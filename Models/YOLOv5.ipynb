{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC8Ve6Q0ji5N"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Yolov5 requirements\n",
        "!pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mJTg6a7Xjn76",
        "outputId": "0f03bc9f-a281-40fa-c296-b53cd5a88a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gitpython>=3.1.30 (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 5))\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 6)) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 7)) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 8)) (4.8.0.76)\n",
            "Collecting Pillow>=10.0.1 (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 9))\n",
            "  Downloading Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 11)) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 12)) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 13)) (1.11.4)\n",
            "Collecting thop>=0.1.1 (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 14))\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 15)) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (0.16.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 17)) (4.66.1)\n",
            "Collecting ultralytics>=8.0.147 (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 18))\n",
            "  Downloading ultralytics-8.0.228-py3-none-any.whl (661 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m661.2/661.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 27)) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 28)) (0.12.2)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 42)) (67.7.2)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 5))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 6)) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 6)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 6)) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 6)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 12)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 12)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 12)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 12)) (2023.11.17)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 15)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 15)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 15)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 15)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 15)) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 15)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 15)) (2.1.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.0.147->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 18)) (9.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 27)) (2023.3.post1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 5))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 15)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 15)) (1.3.0)\n",
            "Installing collected packages: smmap, Pillow, gitdb, thop, gitpython, ultralytics\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.1.0 gitdb-4.0.11 gitpython-3.1.40 smmap-5.0.1 thop-0.1.1.post2209072238 ultralytics-8.0.228\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw64JG9LjoSS",
        "outputId": "4ec66d69-6420-43c8-e918-12f39190ee8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.5.26)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.23.5)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.8.0.76)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.20.3)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (10.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.9 sounddevice-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.display import Image, clear_output\n",
        "from torch.cuda import memory_allocated, empty_cache\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "zgWw86i2joU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch_ver Yolov5\n",
        "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s',\n",
        "                            device='cuda:0' if torch.cuda.is_available() else 'cpu')  # ì˜ˆì¸¡ ëª¨ë¸\n",
        "yolo_model.classes = [0]  # ì˜ˆì¸¡ í´ë˜ìŠ¤ (0 : ì‚¬ëŒ)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFsKcdTFjoPq",
        "outputId": "712ef9f6-caec-42b3-8d9c-22eee21f6b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/hub.py:294: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 ğŸš€ 2023-12-21 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.1M/14.1M [00:00<00:00, 106MB/s] \n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 6\n",
        "EPOCH = 700\n",
        "NUM_LAYERS = 1      # LSTM model: num_layers\n",
        "start_dot = 11      # mp.solutions.pose ì‹œì‘ í¬ì¸íŠ¸ (0: ì–¼êµ´ë¶€í„° ë°œëª©ê¹Œì§€, 11: ì–´ê¹¨ë¶€í„° ë°œëª©ê¹Œì§€)\n",
        "n_CONFIDENCE = 0.3    # MediaPipe Min Detectin confidence check\n",
        "y_CONFIDENCE = 0.3\n",
        "\n",
        "mp_pose = mp.solutions.pose\n",
        "attention_dot = [n for n in range(start_dot, 29)]\n",
        "\n",
        "# ë¼ì¸ ê·¸ë¦¬ê¸°\n",
        "if start_dot == 11:\n",
        "\n",
        "    draw_line = [[11, 13], [13, 15], [15, 21], [15, 19], [15, 17], [17, 19], \\\n",
        "                [12, 14], [14, 16], [16, 22], [16, 20], [16, 18], [18, 20], \\\n",
        "                [23, 25], [25, 27], [24, 26], [26, 28], [11, 12], [11, 23], \\\n",
        "                [23, 24], [12, 24]]\n",
        "    print('Pose : Only Body')\n",
        "\n",
        "else:\n",
        "\n",
        "    draw_line = [[11, 13], [13, 15], [15, 21], [15, 19], [15, 17], [17, 19], \\\n",
        "                [12, 14], [14, 16], [16, 22], [16, 20], [16, 18], [18, 20], \\\n",
        "                [23, 25], [25, 27], [24, 26], [26, 28], [11, 12], [11, 23], \\\n",
        "                [23, 24], [12, 24], [9, 10], [0, 5], [0, 2], [5, 8], [2, 7]]\n",
        "    print('Pose : Face + Body')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxWliFYLjoXU",
        "outputId": "add31622-a477-433e-9e2f-40f9d7234e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pose : Only Body\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°”ìš´ë”© box ì•ˆì—ì„œ media pipe ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "\n",
        "def get_skeleton(video_path, attention_dot, draw_line):\n",
        "    frame_length = 30 # LSTM ëª¨ë¸ì— ë„£ì„ frame ìˆ˜\n",
        "\n",
        "    xy_list_list, xy_list_list_flip = [], []\n",
        "    cv2.destroyAllWindows()\n",
        "    pose = mp_pose.Pose(static_image_mode = True, model_complexity = 1, \\\n",
        "                        enable_segmentation = False, min_detection_confidence = n_CONFIDENCE)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if cap.isOpened():\n",
        "\n",
        "        while True:\n",
        "            ret, img = cap.read()\n",
        "\n",
        "            if ret == True:\n",
        "\n",
        "                \"\"\" Yolo ë°”ìš´ë”© ë°•ìŠ¤ ë° ì¢Œí‘œ ì¶”ì¶œ\"\"\"\n",
        "                img = cv2.resize(img, (640, 640))\n",
        "                res = yolo_model(img)\n",
        "                res_refine = res.pandas().xyxy[0].values\n",
        "                nms_human = len(res_refine)\n",
        "                if nms_human > 0:\n",
        "                    for bbox in res_refine:\n",
        "                        \"\"\"ë°”ìš´ë”© ë°•ìŠ¤ ìƒí•˜ì¢Œìš° í¬ê¸° ì¡°ì ˆ\"\"\"\n",
        "                        xx1, yy1, xx2, yy2 = int(bbox[0])-10, int(bbox[1]), int(bbox[2])+10, int(bbox[3])\n",
        "                        if xx1 < 0:\n",
        "                            xx1 = 0\n",
        "                        elif xx2 > 639:\n",
        "                            xx2 = 639\n",
        "                        if yy1 < 0:\n",
        "                            yy1 = 0\n",
        "                        elif yy2 > 639:\n",
        "                            yy2 = 639\n",
        "\n",
        "                        start_point = (xx1, yy1)\n",
        "                        end_point = (xx2, yy2)\n",
        "\n",
        "                        \"\"\" Yolov5 ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ì•ˆì—ì„œ mediapipe Pose ì¶”ì¶œ\"\"\"\n",
        "                        if bbox[4] > y_CONFIDENCE: # bbox[4] : confidence ë°ì´í„°\n",
        "                            # img = cv2.rectangle(img, start_point, end_point, (0, 0, 255), 2) # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° : ë°ì´í„° ì¶”ì¶œ í™•ì¸ìš©\n",
        "                            c_img = img[yy1:yy2, xx1:xx2] # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ\n",
        "                            results = pose.process(cv2.cvtColor(c_img, cv2.COLOR_BGR2RGB)) # Yolov5 ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ì•ˆì—ì„œ 'mp_pose' ì¢Œí‘œ\n",
        "\n",
        "                            if not results.pose_landmarks: continue\n",
        "                            idx = 0\n",
        "                            draw_line_dic = {}\n",
        "                            xy_list, xy_list_flip = [], []\n",
        "                            # 33 ë°˜ë³µë¬¸ ì§„í–‰ : 33ê°œ ì¤‘ 18ê°œì˜ dot\n",
        "                            for x_and_y in results.pose_landmarks.landmark:\n",
        "                                if idx in attention_dot:\n",
        "                                    xy_list.append(x_and_y.x)\n",
        "                                    xy_list.append(x_and_y.y)\n",
        "                                    xy_list_flip.append(1 - x_and_y.x)\n",
        "                                    xy_list_flip.append(x_and_y.y)\n",
        "\n",
        "                                    x, y = int(x_and_y.x*(xx2-xx1)), int(x_and_y.y*(yy2-yy1))\n",
        "                                    draw_line_dic[idx] = [x, y]\n",
        "                                idx += 1\n",
        "\n",
        "                            if len(xy_list) != len(attention_dot) * 2:\n",
        "                                print('Error : attention_dot ë°ì´í„° ì˜¤ë¥˜')\n",
        "\n",
        "                            xy_list_list.append(xy_list)\n",
        "                            xy_list_list_flip.append(xy_list_flip)\n",
        "\n",
        "                            \"\"\"mediapipe line ê·¸ë¦¬ê¸° ë¶€ë¶„ : ë°ì´í„° ì¶”ì¶œ(dot) í™•ì¸ìš©\"\"\"\n",
        "                            # for line in draw_line:\n",
        "                            #     x1, y1 = draw_line_dic[line[0]][0], draw_line_dic[line[0]][1]\n",
        "                            #     x2, y2 = draw_line_dic[line[1]][0], draw_line_dic[line[1]][1]\n",
        "                            #     c_img = cv2.line(c_img, (x1, y1), (x2, y2), (0, 255, 0), 4)\n",
        "                            # # cv2.imshow('Landmark Image', img)\n",
        "                            # cv2_imshow(img)\n",
        "                            # cv2.waitKey(1)\n",
        "\n",
        "            elif ret == False: break\n",
        "\n",
        "\n",
        "        # ë¶€ì¡±í•œ í”„ë ˆì„ ìˆ˜ ë§ì¶”ê¸°\n",
        "        if len(xy_list_list_flip) < 15:\n",
        "            return False, False\n",
        "        elif len(xy_list_list_flip) < frame_length:\n",
        "            f_ln = frame_length - len(xy_list_list_flip)\n",
        "            for _ in range(f_ln):\n",
        "                xy_list_list.append(xy_list_list[-1])\n",
        "                xy_list_list_flip.append(xy_list_list_flip[-1])\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "    return xy_list_list, xy_list_list_flip"
      ],
      "metadata": {
        "id": "dvOl0b_JjoZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/drive/MyDrive/CCTV/DATA' # dataset ê²½ë¡œ\n",
        "video_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4wfAOiDIjoeC",
        "outputId": "79a08163-8368-4149-e386-67e5deb5806f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/CCTV/DATA'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = []\n",
        "\n",
        "for fold in os.listdir(video_path):\n",
        "    for video_name in os.listdir(video_path + '/' + fold):\n",
        "        if int(video_name.split('_')[3][:2]) >= 30: # video name ì°¸ì¡°\n",
        "            if video_name.split('_')[2] == 'normal': label = 0\n",
        "            else: label = 1\n",
        "            skel_data_n, skel_data_f = get_skeleton('{}/{}'.format(video_path + '/' + fold, video_name), attention_dot, draw_line)\n",
        "            if skel_data_n != False:\n",
        "\n",
        "                seq_list_n = skel_data_n[:30]\n",
        "                seq_list_f = skel_data_f[:30]\n",
        "                raw_data.append({'key':label, 'value':seq_list_n})\n",
        "                raw_data.append({'key':label, 'value':seq_list_f})\n",
        "random.shuffle(raw_data)"
      ],
      "metadata": {
        "id": "sV3MR6c1kFDM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e32e7746-8150-4cc8-bb7c-769d8829bb97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING âš ï¸ NMS time limit 0.550s exceeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nd = 0\n",
        "ad = 0\n",
        "for i in range(len(raw_data)):\n",
        "    if raw_data[i]['key'] == 0:\n",
        "        nd += 1\n",
        "    else:\n",
        "        ad += 1\n",
        "print('normal data:', nd, '| abnormal data:', ad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsdFhvx4jogb",
        "outputId": "c9eec305-4178-499c-a469-3b6b3ea47427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normal data: 62 | abnormal data: 62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgrR73jxjoii",
        "outputId": "2917e160-89d6-46cd-9f6a-d3a794ce6a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, seq_list):\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "        for dic in seq_list :\n",
        "            self.y.append(dic['key'])\n",
        "            self.X.append(dic['value'])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data = self.X[index]\n",
        "        label = self.y[index]\n",
        "        return torch.Tensor(np.array(data)), torch.tensor(np.array(int(label)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)"
      ],
      "metadata": {
        "id": "x9mOzBMAjoku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_ratio = [0.8, 0.1, 0.1]\n",
        "train_len = int(len(raw_data) * split_ratio[0])\n",
        "val_len = int(len(raw_data) * split_ratio[1])\n",
        "test_len = len(raw_data) - train_len - val_len\n",
        "\n",
        "print('{}, {}, {}'.format(train_len, val_len, test_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPETsh2Qjom5",
        "outputId": "36c96ff8-2532-4b30-87b4-6f828954f66d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99, 12, 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MyDataset(raw_data)\n",
        "train_data, valid_data, test_data = random_split(train_dataset, [train_len, val_len, test_len])\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
        "val_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "VidAWu5mjopK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class skeleton_LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(skeleton_LSTM, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size=len(attention_dot) * 2, hidden_size=128, num_layers=NUM_LAYERS, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(input_size=128, hidden_size=256, num_layers=NUM_LAYERS, batch_first=True)\n",
        "        self.lstm3 = nn.LSTM(input_size=256, hidden_size=512, num_layers=NUM_LAYERS, batch_first=True)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.lstm4 = nn.LSTM(input_size=512, hidden_size=256, num_layers=NUM_LAYERS, batch_first=True)\n",
        "        self.lstm5 = nn.LSTM(input_size=256, hidden_size=128, num_layers=NUM_LAYERS, batch_first=True)\n",
        "        self.lstm6 = nn.LSTM(input_size=128, hidden_size=64, num_layers=NUM_LAYERS, batch_first=True)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        self.lstm7 = nn.LSTM(input_size=64, hidden_size=32, num_layers=NUM_LAYERS, batch_first=True)\n",
        "        self.fc = nn.Linear(32,2)\n",
        "\n",
        "    def forward(self, x) :\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x, _ = self.lstm3(x)\n",
        "        x = self.dropout1(x)\n",
        "        x, _ = self.lstm4(x)\n",
        "        x, _ = self.lstm5(x)\n",
        "        x, _ = self.lstm6(x)\n",
        "        x = self.dropout2(x)\n",
        "        x, _ = self.lstm7(x)\n",
        "        x = self.fc(x[:,-1,:]) # x[ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, ì€ë‹‰ ìƒíƒœ í¬ê¸°], [:, -1, :] -> ë§ˆì§€ë§‰ ì‹œê°„ ë‹¨ê³„ë§Œ ì„ íƒ\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "oi5gSneCjorR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_model():\n",
        "    global net, loss_fn, optim\n",
        "    plt.rc('font', size = 10)\n",
        "    net = skeleton_LSTM().to(device)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optim = Adam(net.parameters(), lr=0.0001)\n",
        "\n",
        "# epoch ì¹´ìš´í„° ì´ˆê¸°í™”\n",
        "def init_epoch():\n",
        "    global epoch_cnt\n",
        "    epoch_cnt = 0\n",
        "\n",
        "# ëª¨ë“  Logë¥¼ ì´ˆê¸°í™”\n",
        "def init_log():\n",
        "    global log_stack, iter_log, tloss_log, tacc_log, vloss_log, vacc_log, time_log\n",
        "    plt.rc('font', size = 10)\n",
        "    iter_log, tloss_log, tacc_log, vloss_log, vacc_log = [], [], [], [], []\n",
        "    time_log, log_stack = [], []"
      ],
      "metadata": {
        "id": "Q1b1YSU-joti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def record_train_log(_tloss, _tacc, _time):\n",
        "    # Train Log ê¸°ë¡\n",
        "    time_log.append(_time)\n",
        "    tloss_log.append(_tloss)\n",
        "    tacc_log.append(_tacc)\n",
        "    iter_log.append(epoch_cnt)\n",
        "\n",
        "def record_valid_log(_vloss, _vacc):\n",
        "    # Validation Log ê¸°ë¡\n",
        "    vloss_log.append(_vloss)\n",
        "    vacc_log.append(_vacc)\n",
        "\n",
        "def last(log_list):\n",
        "    # last ì•ˆì˜ ë§ˆì§€ë§‰ ìˆ«ìë¥¼ ë°˜í™˜(print_log í•¨ìˆ˜ì—ì„œ ì‚¬ìš©)\n",
        "    if len(log_list) > 0:\n",
        "        return log_list[len(log_list) - 1]\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def print_log():\n",
        "    # í•™ìŠµ ì¶”ì´ ì¶œë ¥ : ì†Œìˆ«ì  3ìë¦¬ê¹Œì§€\n",
        "    train_loss = round(float(last(tloss_log)), 3)\n",
        "    train_acc = round(float(last(tacc_log)), 3)\n",
        "    val_loss = round(float(last(vloss_log)), 3)\n",
        "    val_acc = round(float(last(vacc_log)), 3)\n",
        "    time_spent = round(float(last(time_log)), 3)\n",
        "\n",
        "    log_str = 'Epoch: {:3} | T_Loss {:5} | T_Acc {:5} | V_Loss {:5} | V_Acc {:5} | {:5}'.format(last(iter_log), train_loss, train_acc, val_loss, val_acc, time_spent)\n",
        "\n",
        "    log_stack.append(log_str)\n",
        "\n",
        "    # í•™ìŠµ ì¶”ì´ ê·¸ë˜í”„ ì¶œë ¥\n",
        "    hist_fig, loss_axis = plt.subplots(figsize=(10, 3), dpi=99)\n",
        "    hist_fig.patch.set_facecolor('white')\n",
        "\n",
        "    # Loss Line êµ¬ì„±\n",
        "    loss_t_line = plt.plot(iter_log, tloss_log, label='Train_Loss', color='red', marker='o')\n",
        "    loss_v_line = plt.plot(iter_log, vloss_log, label='Valid_Loss', color='blue', marker='s')\n",
        "    loss_axis.set_xlabel('epoch')\n",
        "    loss_axis.set_ylabel('loss')\n",
        "\n",
        "    # Acc, Line êµ¬ì„±\n",
        "    acc_axis = loss_axis.twinx()\n",
        "    acc_t_line = acc_axis.plot(iter_log, tacc_log, label='Train_Acc', color='red', marker='+')\n",
        "    acc_v_line = acc_axis.plot(iter_log, vacc_log, label='Valid_Acc', color='blue', marker='x')\n",
        "    acc_axis.set_ylabel('accuracy')\n",
        "\n",
        "    # ê·¸ë˜í”„ ì¶œë ¥\n",
        "    hist_lines = loss_t_line + loss_v_line + acc_t_line + acc_v_line\n",
        "    loss_axis.legend(hist_lines, [l.get_label() for l in hist_lines])\n",
        "    loss_axis.grid()\n",
        "    plt.title('Learning history until epoch {}'.format(last(iter_log)))\n",
        "    plt.draw()\n",
        "\n",
        "    # í…ìŠ¤íŠ¸ ë¡œê·¸ ì¶œë ¥\n",
        "    clear_output(wait=True)\n",
        "    plt.show()\n",
        "    for idx in reversed(range(len(log_stack))):\n",
        "        print(log_stack[idx])"
      ],
      "metadata": {
        "id": "CMttj7oJjovp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_memory():\n",
        "    if device != 'cpu':\n",
        "        empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# í•™ìŠµ ì•Œê³ ë¦¬ì¦˜\n",
        "def epoch(data_loader, mode = 'train'):\n",
        "    global epoch_cnt\n",
        "\n",
        "    # ì‚¬ìš©ë˜ëŠ” ë³€ìˆ˜ ì´ˆê¸°í™”\n",
        "    iter_loss, iter_acc, last_grad_performed = [], [], False\n",
        "\n",
        "    # 1 iteration í•™ìŠµ ì•Œê³ ë¦¬ì¦˜(forë¬¸ì„ ë‚˜ì˜¤ë©´ 1 epoch ì™„ë£Œ)\n",
        "    for _data, _label in data_loader:\n",
        "        data, label = _data.to(device), _label.type(torch.LongTensor).to(device)\n",
        "\n",
        "        # 1. Feed-forward\n",
        "        if mode == 'train':\n",
        "            net.train()\n",
        "        else:\n",
        "            # í•™ìŠµë•Œë§Œ ì“°ì´ëŠ” Dropout, Batch Mormalizationì„ ë¯¸ì‚¬ìš©\n",
        "            net.eval()\n",
        "\n",
        "        result = net(data) # 1 Batchì— ëŒ€í•œ ê²°ê³¼ê°€ ëª¨ë“  Classì— ëŒ€í•œ í™•ë¥ ê°’ìœ¼ë¡œ\n",
        "        _, out = torch.max(result, 1) # resultì—ì„œ ìµœëŒ€ í™•ë¥ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ class ë„ì¶œ( _ : ê°’ ë¶€ë¶„ì€ í•„ìš” ì—†ìŒ, out : index ì¤‘ ê°€ì¥ í° í•˜ë‚˜ì˜ ë°ì´í„°)\n",
        "\n",
        "        # 2. Loss ê³„ì‚°\n",
        "        loss = loss_fn(result, label) # GT ì™€ Label ë¹„êµí•˜ì—¬ Loss ì‚°ì •\n",
        "        iter_loss.append(loss.item()) # í•™ìŠµ ì¶”ì´ë¥¼ ìœ„í•˜ì—¬ Lossë¥¼ ê¸°ë¡\n",
        "\n",
        "        # 3. ì—­ì „íŒŒ í•™ìŠµ í›„ Gradient Descent\n",
        "        if mode == 'train':\n",
        "            optim.zero_grad() # ë¯¸ë¶„ì„ í†µí•´ ì–»ì€ ê¸°ìš¸ê¸°ë¥¼ ì´ˆê¸°í™” for ë‹¤ìŒ epoch\n",
        "            loss.backward() # ì—­ì „íŒŒ í•™ìŠµ\n",
        "            optim.step() # Gradient Descent ìˆ˜í–‰\n",
        "            last_grad_performed = True # forë¬¸ì„ ë‚˜ê°€ë©´ epoch ì¹´ìš´í„° += 1\n",
        "\n",
        "        # 4. ì •í™•ë„ ê³„ì‚°\n",
        "        acc_partial = (out == label).float().sum() # GT == Label ì¸ ê°œìˆ˜\n",
        "        acc_partial = acc_partial / len(label) # ( TP / (TP + TM)) í•´ì„œ ì •í™•ë„ ì‚°ì¶œ\n",
        "        iter_acc.append(acc_partial.item()) # í•™ìŠµ ì¶”ì´ë¥¼ ìœ„í•˜ì—¬ Acc. ê¸°ë¡\n",
        "\n",
        "    # ì—­ì „íŒŒ í•™ìŠµ í›„ Epoch ì¹´ìš´í„° += 1\n",
        "    if last_grad_performed:\n",
        "        epoch_cnt += 1\n",
        "\n",
        "    clear_memory()\n",
        "\n",
        "    # lossì™€ accì˜ í‰ê· ê°’ for í•™ìŠµì¶”ì´ ê·¸ë˜í”„, ëª¨ë“  GTì™€ Label ê°’ for ì»¨í“¨ì „ ë§¤íŠ¸ë¦­ìŠ¤\n",
        "    return np.average(iter_loss), np.average(iter_acc)\n",
        "\n",
        "def epoch_not_finished():\n",
        "    # ì—í­ì´ ëë‚¨ì„ ì•Œë¦¼\n",
        "    return epoch_cnt < maximum_epoch"
      ],
      "metadata": {
        "id": "_DWiQY9Wjox5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# í•™ìŠµ"
      ],
      "metadata": {
        "id": "5_YzO6OOjo0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training initialization\n",
        "init_model()\n",
        "init_epoch()\n",
        "init_log()\n",
        "maximum_epoch = EPOCH\n",
        "\n",
        "# Training iteration\n",
        "\n",
        "while epoch_not_finished():\n",
        "    start_time = time.time()\n",
        "\n",
        "    tloss, tacc = epoch(train_loader, mode = 'train')\n",
        "\n",
        "    end_time = time.time()\n",
        "    time_taken = end_time - start_time\n",
        "    record_train_log(tloss, tacc, time_taken)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        vloss, vacc = epoch(val_loader, mode = 'val')\n",
        "        record_valid_log(vloss, vacc)\n",
        "\n",
        "    print_log()\n",
        "\n",
        "print('\\n completed!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g-CAMbQjo2B",
        "outputId": "c39d7256-608d-4756-84c9-3b9b30a51ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 700 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.313 | V_Acc 0.833 | 2.734\n",
            "Epoch: 699 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.323 | V_Acc 0.833 | 2.816\n",
            "Epoch: 698 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.322 | V_Acc 0.833 |  2.65\n",
            "Epoch: 697 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.315 | V_Acc 0.833 |  3.11\n",
            "Epoch: 696 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.333 | V_Acc 0.833 | 2.545\n",
            "Epoch: 695 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.345 | V_Acc 0.833 | 2.734\n",
            "Epoch: 694 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.347 | V_Acc 0.833 | 2.967\n",
            "Epoch: 693 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.346 | V_Acc 0.833 | 2.692\n",
            "Epoch: 692 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.335 | V_Acc 0.833 | 2.806\n",
            "Epoch: 691 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.341 | V_Acc 0.833 | 2.733\n",
            "Epoch: 690 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.347 | V_Acc 0.833 | 2.704\n",
            "Epoch: 689 | T_Loss 0.004 | T_Acc   1.0 | V_Loss  1.35 | V_Acc 0.833 | 3.202\n",
            "Epoch: 688 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.356 | V_Acc 0.833 | 2.696\n",
            "Epoch: 687 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.352 | V_Acc 0.833 | 2.711\n",
            "Epoch: 686 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.363 | V_Acc 0.833 | 2.703\n",
            "Epoch: 685 | T_Loss 0.004 | T_Acc   1.0 | V_Loss  1.37 | V_Acc 0.833 |  2.49\n",
            "Epoch: 684 | T_Loss 0.004 | T_Acc   1.0 | V_Loss  1.36 | V_Acc 0.833 |  3.11\n",
            "Epoch: 683 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.363 | V_Acc 0.833 | 2.675\n",
            "Epoch: 682 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.355 | V_Acc 0.833 |  2.47\n",
            "Epoch: 681 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.361 | V_Acc 0.833 | 3.078\n",
            "Epoch: 680 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.358 | V_Acc 0.833 | 2.631\n",
            "Epoch: 679 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.363 | V_Acc 0.833 | 2.645\n",
            "Epoch: 678 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.364 | V_Acc 0.833 | 2.648\n",
            "Epoch: 677 | T_Loss 0.004 | T_Acc   1.0 | V_Loss  1.36 | V_Acc 0.833 | 2.584\n",
            "Epoch: 676 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.363 | V_Acc 0.833 | 3.117\n",
            "Epoch: 675 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.366 | V_Acc 0.833 | 2.658\n",
            "Epoch: 674 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.357 | V_Acc 0.833 | 2.648\n",
            "Epoch: 673 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.362 | V_Acc 0.833 | 3.023\n",
            "Epoch: 672 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.359 | V_Acc 0.833 |   2.5\n",
            "Epoch: 671 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.362 | V_Acc 0.833 | 2.757\n",
            "Epoch: 670 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.361 | V_Acc 0.833 | 2.672\n",
            "Epoch: 669 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.361 | V_Acc 0.833 | 2.523\n",
            "Epoch: 668 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.356 | V_Acc 0.833 | 3.069\n",
            "Epoch: 667 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.355 | V_Acc 0.833 | 2.502\n",
            "Epoch: 666 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.357 | V_Acc 0.833 | 2.651\n",
            "Epoch: 665 | T_Loss 0.004 | T_Acc   1.0 | V_Loss  1.35 | V_Acc 0.833 | 2.902\n",
            "Epoch: 664 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.359 | V_Acc 0.833 | 2.446\n",
            "Epoch: 663 | T_Loss 0.004 | T_Acc   1.0 | V_Loss  1.36 | V_Acc 0.833 | 2.753\n",
            "Epoch: 662 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.347 | V_Acc 0.833 | 2.547\n",
            "Epoch: 661 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.357 | V_Acc 0.833 |  2.43\n",
            "Epoch: 660 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.347 | V_Acc 0.833 | 3.045\n",
            "Epoch: 659 | T_Loss 0.004 | T_Acc   1.0 | V_Loss  1.33 | V_Acc 0.833 | 2.602\n",
            "Epoch: 658 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.319 | V_Acc 0.833 | 2.525\n",
            "Epoch: 657 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.312 | V_Acc 0.833 | 3.023\n",
            "Epoch: 656 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.311 | V_Acc 0.833 | 2.431\n",
            "Epoch: 655 | T_Loss 0.004 | T_Acc   1.0 | V_Loss  1.31 | V_Acc 0.833 | 2.609\n",
            "Epoch: 654 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.307 | V_Acc 0.833 | 2.471\n",
            "Epoch: 653 | T_Loss 0.004 | T_Acc   1.0 | V_Loss   1.3 | V_Acc 0.833 | 2.394\n",
            "Epoch: 652 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.294 | V_Acc 0.833 | 3.084\n",
            "Epoch: 651 | T_Loss 0.004 | T_Acc   1.0 | V_Loss 1.293 | V_Acc 0.833 | 2.999\n",
            "Epoch: 650 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.298 | V_Acc 0.833 | 2.473\n",
            "Epoch: 649 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.305 | V_Acc 0.833 | 2.581\n",
            "Epoch: 648 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.304 | V_Acc 0.833 | 2.477\n",
            "Epoch: 647 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.306 | V_Acc 0.833 | 2.395\n",
            "Epoch: 646 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.307 | V_Acc 0.833 | 2.968\n",
            "Epoch: 645 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.306 | V_Acc 0.833 | 2.422\n",
            "Epoch: 644 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.302 | V_Acc 0.833 | 2.399\n",
            "Epoch: 643 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.297 | V_Acc 0.833 | 3.037\n",
            "Epoch: 642 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.289 | V_Acc 0.833 | 2.535\n",
            "Epoch: 641 | T_Loss 0.005 | T_Acc   1.0 | V_Loss  1.28 | V_Acc 0.833 | 2.391\n",
            "Epoch: 640 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.284 | V_Acc 0.833 | 2.817\n",
            "Epoch: 639 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.283 | V_Acc 0.833 | 2.473\n",
            "Epoch: 638 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.284 | V_Acc 0.833 | 2.602\n",
            "Epoch: 637 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.285 | V_Acc 0.833 | 2.467\n",
            "Epoch: 636 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.278 | V_Acc 0.833 | 2.421\n",
            "Epoch: 635 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.276 | V_Acc 0.833 | 2.922\n",
            "Epoch: 634 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.286 | V_Acc 0.833 |  2.33\n",
            "Epoch: 633 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.294 | V_Acc 0.833 | 2.369\n",
            "Epoch: 632 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.291 | V_Acc 0.833 | 2.956\n",
            "Epoch: 631 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.279 | V_Acc 0.833 | 2.438\n",
            "Epoch: 630 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.277 | V_Acc 0.833 | 2.318\n",
            "Epoch: 629 | T_Loss 0.005 | T_Acc   1.0 | V_Loss  1.27 | V_Acc 0.833 | 2.978\n",
            "Epoch: 628 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.267 | V_Acc 0.833 |   2.5\n",
            "Epoch: 627 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.267 | V_Acc 0.833 | 2.416\n",
            "Epoch: 626 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.271 | V_Acc 0.833 | 2.799\n",
            "Epoch: 625 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.271 | V_Acc 0.833 | 2.446\n",
            "Epoch: 624 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.269 | V_Acc 0.833 |  2.55\n",
            "Epoch: 623 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.262 | V_Acc 0.833 | 2.566\n",
            "Epoch: 622 | T_Loss 0.005 | T_Acc   1.0 | V_Loss  1.26 | V_Acc 0.833 | 2.368\n",
            "Epoch: 621 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.257 | V_Acc 0.833 | 2.781\n",
            "Epoch: 620 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.256 | V_Acc 0.833 | 2.461\n",
            "Epoch: 619 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.255 | V_Acc 0.833 | 2.477\n",
            "Epoch: 618 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.256 | V_Acc 0.833 | 2.865\n",
            "Epoch: 617 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.254 | V_Acc 0.833 |  2.32\n",
            "Epoch: 616 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.253 | V_Acc 0.833 | 2.308\n",
            "Epoch: 615 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.253 | V_Acc 0.833 | 2.909\n",
            "Epoch: 614 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.254 | V_Acc 0.833 | 2.454\n",
            "Epoch: 613 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.253 | V_Acc 0.833 | 2.461\n",
            "Epoch: 612 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.251 | V_Acc 0.833 | 2.893\n",
            "Epoch: 611 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.252 | V_Acc 0.833 | 2.473\n",
            "Epoch: 610 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.246 | V_Acc 0.833 | 2.452\n",
            "Epoch: 609 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.235 | V_Acc 0.833 | 2.747\n",
            "Epoch: 608 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.239 | V_Acc 0.833 | 2.447\n",
            "Epoch: 607 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.239 | V_Acc 0.833 | 2.346\n",
            "Epoch: 606 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.244 | V_Acc 0.833 |  2.61\n",
            "Epoch: 605 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.247 | V_Acc 0.833 |  2.26\n",
            "Epoch: 604 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.246 | V_Acc 0.833 | 2.446\n",
            "Epoch: 603 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.243 | V_Acc 0.833 | 2.369\n",
            "Epoch: 602 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.239 | V_Acc 0.833 | 2.254\n",
            "Epoch: 601 | T_Loss 0.005 | T_Acc   1.0 | V_Loss  1.24 | V_Acc 0.833 | 2.506\n",
            "Epoch: 600 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.236 | V_Acc 0.833 |  2.49\n",
            "Epoch: 599 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.232 | V_Acc 0.833 | 2.421\n",
            "Epoch: 598 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.234 | V_Acc 0.833 | 2.529\n",
            "Epoch: 597 | T_Loss 0.005 | T_Acc   1.0 | V_Loss  1.23 | V_Acc 0.833 |   2.3\n",
            "Epoch: 596 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.225 | V_Acc 0.833 | 2.381\n",
            "Epoch: 595 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.225 | V_Acc 0.833 | 2.554\n",
            "Epoch: 594 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.225 | V_Acc 0.833 | 2.309\n",
            "Epoch: 593 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.221 | V_Acc 0.833 | 2.393\n",
            "Epoch: 592 | T_Loss 0.005 | T_Acc   1.0 | V_Loss  1.22 | V_Acc 0.833 | 2.601\n",
            "Epoch: 591 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.219 | V_Acc 0.833 | 2.273\n",
            "Epoch: 590 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.218 | V_Acc 0.833 | 2.365\n",
            "Epoch: 589 | T_Loss 0.005 | T_Acc   1.0 | V_Loss  1.22 | V_Acc 0.833 | 2.667\n",
            "Epoch: 588 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.219 | V_Acc 0.833 |  2.24\n",
            "Epoch: 587 | T_Loss 0.005 | T_Acc   1.0 | V_Loss  1.22 | V_Acc 0.833 | 2.364\n",
            "Epoch: 586 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.221 | V_Acc 0.833 | 2.706\n",
            "Epoch: 585 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.229 | V_Acc 0.833 | 2.371\n",
            "Epoch: 584 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.233 | V_Acc 0.833 | 2.367\n",
            "Epoch: 583 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.227 | V_Acc 0.833 | 2.708\n",
            "Epoch: 582 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.223 | V_Acc 0.833 | 2.292\n",
            "Epoch: 581 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.217 | V_Acc 0.833 | 2.203\n",
            "Epoch: 580 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.211 | V_Acc 0.833 | 2.728\n",
            "Epoch: 579 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.201 | V_Acc 0.833 | 2.186\n",
            "Epoch: 578 | T_Loss 0.005 | T_Acc   1.0 | V_Loss   1.2 | V_Acc 0.833 | 2.385\n",
            "Epoch: 577 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.198 | V_Acc 0.833 | 2.719\n",
            "Epoch: 576 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.199 | V_Acc 0.833 | 2.188\n",
            "Epoch: 575 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.197 | V_Acc 0.833 | 2.337\n",
            "Epoch: 574 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.194 | V_Acc 0.833 | 2.731\n",
            "Epoch: 573 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.193 | V_Acc 0.833 | 2.242\n",
            "Epoch: 572 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.189 | V_Acc 0.833 | 2.309\n",
            "Epoch: 571 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.185 | V_Acc 0.833 | 2.671\n",
            "Epoch: 570 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.177 | V_Acc 0.833 | 2.307\n",
            "Epoch: 569 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.172 | V_Acc 0.833 | 2.302\n",
            "Epoch: 568 | T_Loss 0.005 | T_Acc   1.0 | V_Loss  1.17 | V_Acc 0.833 | 2.672\n",
            "Epoch: 567 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.167 | V_Acc 0.833 | 2.158\n",
            "Epoch: 566 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.172 | V_Acc 0.833 | 2.242\n",
            "Epoch: 565 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.168 | V_Acc 0.833 | 2.691\n",
            "Epoch: 564 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.165 | V_Acc 0.833 | 2.149\n",
            "Epoch: 563 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.166 | V_Acc 0.833 | 2.291\n",
            "Epoch: 562 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.173 | V_Acc 0.833 | 2.556\n",
            "Epoch: 561 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.172 | V_Acc 0.833 |   2.3\n",
            "Epoch: 560 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.165 | V_Acc 0.833 | 2.294\n",
            "Epoch: 559 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.167 | V_Acc 0.833 | 2.417\n",
            "Epoch: 558 | T_Loss 0.005 | T_Acc   1.0 | V_Loss  1.17 | V_Acc 0.833 | 2.232\n",
            "Epoch: 557 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.172 | V_Acc 0.833 | 2.266\n",
            "Epoch: 556 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.173 | V_Acc 0.833 | 2.315\n",
            "Epoch: 555 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.174 | V_Acc 0.833 | 2.251\n",
            "Epoch: 554 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.172 | V_Acc 0.833 | 2.204\n",
            "Epoch: 553 | T_Loss 0.005 | T_Acc   1.0 | V_Loss  1.17 | V_Acc 0.833 | 2.128\n",
            "Epoch: 552 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.169 | V_Acc 0.833 | 2.447\n",
            "Epoch: 551 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.166 | V_Acc 0.833 | 2.249\n",
            "Epoch: 550 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.166 | V_Acc 0.833 | 2.254\n",
            "Epoch: 549 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.168 | V_Acc 0.833 | 2.635\n",
            "Epoch: 548 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.163 | V_Acc 0.833 | 2.191\n",
            "Epoch: 547 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.166 | V_Acc 0.833 | 2.263\n",
            "Epoch: 546 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.166 | V_Acc 0.833 | 2.642\n",
            "Epoch: 545 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.164 | V_Acc 0.833 | 2.288\n",
            "Epoch: 544 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.163 | V_Acc 0.833 | 2.152\n",
            "Epoch: 543 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.168 | V_Acc 0.833 | 2.639\n",
            "Epoch: 542 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.168 | V_Acc 0.833 | 2.098\n",
            "Epoch: 541 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.162 | V_Acc 0.833 |  2.24\n",
            "Epoch: 540 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.153 | V_Acc 0.833 | 2.663\n",
            "Epoch: 539 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.149 | V_Acc 0.833 | 2.237\n",
            "Epoch: 538 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.147 | V_Acc 0.833 | 2.096\n",
            "Epoch: 537 | T_Loss 0.005 | T_Acc   1.0 | V_Loss  1.14 | V_Acc 0.833 | 2.616\n",
            "Epoch: 536 | T_Loss 0.006 | T_Acc   1.0 | V_Loss  1.14 | V_Acc 0.833 | 2.208\n",
            "Epoch: 535 | T_Loss 0.005 | T_Acc   1.0 | V_Loss 1.143 | V_Acc 0.833 | 2.224\n",
            "Epoch: 534 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.141 | V_Acc 0.833 | 2.837\n",
            "Epoch: 533 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.137 | V_Acc 0.833 |  2.51\n",
            "Epoch: 532 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.134 | V_Acc 0.833 | 2.071\n",
            "Epoch: 531 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.134 | V_Acc 0.833 | 2.058\n",
            "Epoch: 530 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.127 | V_Acc 0.833 |  2.57\n",
            "Epoch: 529 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.127 | V_Acc 0.833 | 2.116\n",
            "Epoch: 528 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.128 | V_Acc 0.833 | 2.188\n",
            "Epoch: 527 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.132 | V_Acc 0.833 | 2.594\n",
            "Epoch: 526 | T_Loss 0.006 | T_Acc   1.0 | V_Loss  1.13 | V_Acc 0.833 | 2.087\n",
            "Epoch: 525 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.129 | V_Acc 0.833 | 2.054\n",
            "Epoch: 524 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.134 | V_Acc 0.833 | 2.587\n",
            "Epoch: 523 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.134 | V_Acc 0.833 |  2.18\n",
            "Epoch: 522 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.131 | V_Acc 0.833 | 2.168\n",
            "Epoch: 521 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.133 | V_Acc 0.833 | 2.263\n",
            "Epoch: 520 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.131 | V_Acc 0.833 | 2.121\n",
            "Epoch: 519 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.125 | V_Acc 0.833 | 2.039\n",
            "Epoch: 518 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.126 | V_Acc 0.833 | 2.166\n",
            "Epoch: 517 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.125 | V_Acc 0.833 | 2.568\n",
            "Epoch: 516 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.114 | V_Acc 0.833 | 2.146\n",
            "Epoch: 515 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.112 | V_Acc 0.833 | 2.159\n",
            "Epoch: 514 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.107 | V_Acc 0.833 | 2.526\n",
            "Epoch: 513 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.113 | V_Acc 0.833 |  2.15\n",
            "Epoch: 512 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.116 | V_Acc 0.833 | 2.137\n",
            "Epoch: 511 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.109 | V_Acc 0.833 | 2.379\n",
            "Epoch: 510 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.105 | V_Acc 0.833 | 2.165\n",
            "Epoch: 509 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.103 | V_Acc 0.833 | 2.151\n",
            "Epoch: 508 | T_Loss 0.006 | T_Acc   1.0 | V_Loss   1.1 | V_Acc 0.833 | 2.034\n",
            "Epoch: 507 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.096 | V_Acc 0.833 | 2.496\n",
            "Epoch: 506 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.094 | V_Acc 0.833 | 2.127\n",
            "Epoch: 505 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.091 | V_Acc 0.833 | 2.111\n",
            "Epoch: 504 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.087 | V_Acc 0.833 |  2.47\n",
            "Epoch: 503 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.087 | V_Acc 0.833 | 1.993\n",
            "Epoch: 502 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.093 | V_Acc 0.833 | 2.129\n",
            "Epoch: 501 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.089 | V_Acc 0.833 | 2.401\n",
            "Epoch: 500 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.083 | V_Acc 0.833 | 2.126\n",
            "Epoch: 499 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.081 | V_Acc 0.833 | 1.975\n",
            "Epoch: 498 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.079 | V_Acc 0.833 |   2.1\n",
            "Epoch: 497 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.081 | V_Acc 0.833 | 2.445\n",
            "Epoch: 496 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.083 | V_Acc 0.833 |  1.96\n",
            "Epoch: 495 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.083 | V_Acc 0.833 | 2.078\n",
            "Epoch: 494 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.083 | V_Acc 0.833 | 2.447\n",
            "Epoch: 493 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.079 | V_Acc 0.833 | 1.934\n",
            "Epoch: 492 | T_Loss 0.006 | T_Acc   1.0 | V_Loss  1.08 | V_Acc 0.833 | 1.997\n",
            "Epoch: 491 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.077 | V_Acc 0.833 | 2.237\n",
            "Epoch: 490 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.075 | V_Acc 0.833 | 2.154\n",
            "Epoch: 489 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.067 | V_Acc 0.833 | 2.055\n",
            "Epoch: 488 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.066 | V_Acc 0.833 | 2.059\n",
            "Epoch: 487 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.062 | V_Acc 0.833 | 2.416\n",
            "Epoch: 486 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.064 | V_Acc 0.833 | 2.072\n",
            "Epoch: 485 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.068 | V_Acc 0.833 | 1.969\n",
            "Epoch: 484 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.064 | V_Acc 0.833 | 2.382\n",
            "Epoch: 483 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.066 | V_Acc 0.833 | 1.977\n",
            "Epoch: 482 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.053 | V_Acc 0.833 | 1.909\n",
            "Epoch: 481 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.042 | V_Acc 0.833 | 2.023\n",
            "Epoch: 480 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.038 | V_Acc 0.833 | 2.411\n",
            "Epoch: 479 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.035 | V_Acc 0.833 | 2.015\n",
            "Epoch: 478 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.033 | V_Acc 0.833 | 1.948\n",
            "Epoch: 477 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 1.031 | V_Acc 0.833 | 2.336\n",
            "Epoch: 476 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.022 | V_Acc 0.833 | 2.044\n",
            "Epoch: 475 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.019 | V_Acc 0.833 | 1.876\n",
            "Epoch: 474 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.014 | V_Acc 0.833 | 1.942\n",
            "Epoch: 473 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.009 | V_Acc 0.833 | 2.356\n",
            "Epoch: 472 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 1.008 | V_Acc 0.833 | 1.863\n",
            "Epoch: 471 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 1.007 | V_Acc 0.833 | 2.021\n",
            "Epoch: 470 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 1.003 | V_Acc 0.833 | 2.335\n",
            "Epoch: 469 | T_Loss 0.006 | T_Acc   1.0 | V_Loss   1.0 | V_Acc 0.833 | 2.015\n",
            "Epoch: 468 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 0.997 | V_Acc 0.833 | 2.013\n",
            "Epoch: 467 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 0.991 | V_Acc 0.833 |   1.9\n",
            "Epoch: 466 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 0.991 | V_Acc 0.833 | 2.386\n",
            "Epoch: 465 | T_Loss 0.006 | T_Acc   1.0 | V_Loss 0.985 | V_Acc 0.833 | 1.901\n",
            "Epoch: 464 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.981 | V_Acc 0.833 | 1.863\n",
            "Epoch: 463 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.971 | V_Acc 0.833 | 2.167\n",
            "Epoch: 462 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.963 | V_Acc 0.833 | 2.088\n",
            "Epoch: 461 | T_Loss 0.007 | T_Acc   1.0 | V_Loss  0.96 | V_Acc 0.833 | 1.972\n",
            "Epoch: 460 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.961 | V_Acc 0.833 | 1.881\n",
            "Epoch: 459 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.962 | V_Acc 0.833 | 2.303\n",
            "Epoch: 458 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.953 | V_Acc 0.833 | 1.984\n",
            "Epoch: 457 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.946 | V_Acc 0.833 | 1.878\n",
            "Epoch: 456 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.942 | V_Acc 0.833 | 1.961\n",
            "Epoch: 455 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.936 | V_Acc 0.833 |  2.14\n",
            "Epoch: 454 | T_Loss 0.007 | T_Acc   1.0 | V_Loss  0.93 | V_Acc 0.833 | 1.879\n",
            "Epoch: 453 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.928 | V_Acc 0.833 | 1.965\n",
            "Epoch: 452 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.921 | V_Acc 0.833 | 2.222\n",
            "Epoch: 451 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.916 | V_Acc 0.833 | 1.842\n",
            "Epoch: 450 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.911 | V_Acc 0.833 | 1.913\n",
            "Epoch: 449 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.909 | V_Acc 0.833 | 1.936\n",
            "Epoch: 448 | T_Loss 0.007 | T_Acc   1.0 | V_Loss  0.91 | V_Acc 0.833 | 2.279\n",
            "Epoch: 447 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.911 | V_Acc 0.833 | 1.939\n",
            "Epoch: 446 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.908 | V_Acc 0.833 | 1.927\n",
            "Epoch: 445 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.903 | V_Acc 0.833 | 1.987\n",
            "Epoch: 444 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.897 | V_Acc 0.833 | 2.126\n",
            "Epoch: 443 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.886 | V_Acc 0.833 | 1.909\n",
            "Epoch: 442 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.885 | V_Acc 0.833 | 1.847\n",
            "Epoch: 441 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.879 | V_Acc 0.833 | 2.284\n",
            "Epoch: 440 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.874 | V_Acc 0.833 | 1.932\n",
            "Epoch: 439 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.867 | V_Acc 0.833 | 1.895\n",
            "Epoch: 438 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.855 | V_Acc 0.833 | 1.865\n",
            "Epoch: 437 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.848 | V_Acc 0.833 |  2.23\n",
            "Epoch: 436 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.842 | V_Acc 0.833 | 1.905\n",
            "Epoch: 435 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.836 | V_Acc 0.833 | 1.893\n",
            "Epoch: 434 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.831 | V_Acc 0.833 | 1.875\n",
            "Epoch: 433 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.829 | V_Acc 0.833 | 2.158\n",
            "Epoch: 432 | T_Loss 0.007 | T_Acc   1.0 | V_Loss  0.82 | V_Acc 0.833 | 1.803\n",
            "Epoch: 431 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.811 | V_Acc 0.833 | 1.876\n",
            "Epoch: 430 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.809 | V_Acc 0.833 | 2.178\n",
            "Epoch: 429 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.808 | V_Acc 0.833 | 1.824\n",
            "Epoch: 428 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.798 | V_Acc 0.833 |  1.87\n",
            "Epoch: 427 | T_Loss 0.008 | T_Acc   1.0 | V_Loss 0.789 | V_Acc 0.833 | 1.746\n",
            "Epoch: 426 | T_Loss 0.008 | T_Acc   1.0 | V_Loss 0.778 | V_Acc 0.833 | 2.176\n",
            "Epoch: 425 | T_Loss 0.008 | T_Acc   1.0 | V_Loss 0.777 | V_Acc 0.833 | 1.817\n",
            "Epoch: 424 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.778 | V_Acc 0.833 | 1.879\n",
            "Epoch: 423 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.772 | V_Acc 0.833 | 1.797\n",
            "Epoch: 422 | T_Loss 0.007 | T_Acc   1.0 | V_Loss 0.764 | V_Acc 0.833 | 2.188\n",
            "Epoch: 421 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.755 | V_Acc 0.833 | 1.826\n",
            "Epoch: 420 | T_Loss 0.009 | T_Acc  0.99 | V_Loss 0.745 | V_Acc 0.833 | 1.843\n",
            "Epoch: 419 | T_Loss 0.008 | T_Acc   1.0 | V_Loss 0.743 | V_Acc 0.833 | 1.765\n",
            "Epoch: 418 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.734 | V_Acc 0.833 | 2.172\n",
            "Epoch: 417 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.731 | V_Acc 0.833 | 1.719\n",
            "Epoch: 416 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.734 | V_Acc 0.833 | 1.828\n",
            "Epoch: 415 | T_Loss 0.008 | T_Acc   1.0 | V_Loss 0.726 | V_Acc 0.833 | 1.849\n",
            "Epoch: 414 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.712 | V_Acc 0.833 | 2.117\n",
            "Epoch: 413 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.704 | V_Acc 0.833 | 1.695\n",
            "Epoch: 412 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.697 | V_Acc 0.833 | 1.758\n",
            "Epoch: 411 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.687 | V_Acc 0.833 |  1.95\n",
            "Epoch: 410 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.681 | V_Acc 0.833 | 2.033\n",
            "Epoch: 409 | T_Loss 0.008 | T_Acc   1.0 | V_Loss 0.681 | V_Acc 0.833 | 1.794\n",
            "Epoch: 408 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.677 | V_Acc 0.833 |  1.75\n",
            "Epoch: 407 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.662 | V_Acc 0.833 | 2.084\n",
            "Epoch: 406 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.649 | V_Acc 0.833 | 1.878\n",
            "Epoch: 405 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.642 | V_Acc 0.833 | 1.806\n",
            "Epoch: 404 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.638 | V_Acc 0.833 |  1.79\n",
            "Epoch: 403 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.629 | V_Acc 0.833 | 2.123\n",
            "Epoch: 402 | T_Loss 0.009 | T_Acc  0.99 | V_Loss 0.616 | V_Acc 0.833 | 1.836\n",
            "Epoch: 401 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.611 | V_Acc 0.833 | 1.796\n",
            "Epoch: 400 | T_Loss 0.009 | T_Acc  0.99 | V_Loss 0.607 | V_Acc 0.833 | 1.792\n",
            "Epoch: 399 | T_Loss 0.009 | T_Acc  0.99 | V_Loss 0.598 | V_Acc 0.833 | 2.133\n",
            "Epoch: 398 | T_Loss 0.009 | T_Acc  0.99 | V_Loss 0.586 | V_Acc 0.833 | 1.806\n",
            "Epoch: 397 | T_Loss 0.008 | T_Acc  0.99 | V_Loss 0.589 | V_Acc 0.833 |  1.79\n",
            "Epoch: 396 | T_Loss  0.01 | T_Acc  0.99 | V_Loss 0.578 | V_Acc 0.833 | 1.647\n",
            "Epoch: 395 | T_Loss 0.009 | T_Acc  0.99 | V_Loss 0.571 | V_Acc 0.833 | 2.073\n",
            "Epoch: 394 | T_Loss  0.01 | T_Acc  0.99 | V_Loss 0.558 | V_Acc 0.833 |   1.8\n",
            "Epoch: 393 | T_Loss 0.009 | T_Acc  0.99 | V_Loss 0.554 | V_Acc 0.833 | 2.066\n",
            "Epoch: 392 | T_Loss  0.01 | T_Acc  0.99 | V_Loss 0.547 | V_Acc 0.833 | 2.068\n",
            "Epoch: 391 | T_Loss 0.009 | T_Acc  0.99 | V_Loss 0.544 | V_Acc 0.833 | 2.075\n",
            "Epoch: 390 | T_Loss 0.009 | T_Acc  0.99 | V_Loss 0.535 | V_Acc 0.833 | 1.773\n",
            "Epoch: 389 | T_Loss 0.009 | T_Acc  0.99 | V_Loss 0.531 | V_Acc 0.833 | 1.772\n",
            "Epoch: 388 | T_Loss  0.01 | T_Acc  0.99 | V_Loss 0.523 | V_Acc 0.833 | 1.755\n",
            "Epoch: 387 | T_Loss  0.01 | T_Acc  0.99 | V_Loss 0.513 | V_Acc 0.833 | 2.067\n",
            "Epoch: 386 | T_Loss  0.01 | T_Acc  0.99 | V_Loss 0.508 | V_Acc 0.833 | 1.666\n",
            "Epoch: 385 | T_Loss  0.01 | T_Acc  0.99 | V_Loss 0.508 | V_Acc 0.833 |  1.76\n",
            "Epoch: 384 | T_Loss 0.009 | T_Acc  0.99 | V_Loss  0.51 | V_Acc 0.833 | 1.752\n",
            "Epoch: 383 | T_Loss  0.01 | T_Acc  0.99 | V_Loss 0.504 | V_Acc 0.833 | 2.061\n",
            "Epoch: 382 | T_Loss  0.01 | T_Acc  0.99 | V_Loss   0.5 | V_Acc 0.833 | 1.728\n",
            "Epoch: 381 | T_Loss  0.01 | T_Acc  0.99 | V_Loss 0.491 | V_Acc 0.833 | 1.695\n",
            "Epoch: 380 | T_Loss 0.011 | T_Acc  0.99 | V_Loss 0.486 | V_Acc 0.833 | 1.735\n",
            "Epoch: 379 | T_Loss  0.01 | T_Acc  0.99 | V_Loss 0.483 | V_Acc 0.833 | 2.037\n",
            "Epoch: 378 | T_Loss  0.01 | T_Acc  0.99 | V_Loss 0.483 | V_Acc 0.833 | 1.717\n",
            "Epoch: 377 | T_Loss  0.01 | T_Acc  0.99 | V_Loss  0.48 | V_Acc 0.833 | 1.737\n",
            "Epoch: 376 | T_Loss 0.011 | T_Acc  0.99 | V_Loss 0.473 | V_Acc 0.833 | 1.709\n",
            "Epoch: 375 | T_Loss  0.01 | T_Acc  0.99 | V_Loss  0.47 | V_Acc 0.917 | 2.015\n",
            "Epoch: 374 | T_Loss 0.011 | T_Acc  0.99 | V_Loss 0.468 | V_Acc 0.917 | 1.711\n",
            "Epoch: 373 | T_Loss 0.011 | T_Acc  0.99 | V_Loss 0.464 | V_Acc 0.917 | 1.694\n",
            "Epoch: 372 | T_Loss 0.011 | T_Acc  0.99 | V_Loss 0.459 | V_Acc 0.917 | 1.727\n",
            "Epoch: 371 | T_Loss 0.012 | T_Acc  0.99 | V_Loss 0.458 | V_Acc 0.917 | 2.001\n",
            "Epoch: 370 | T_Loss 0.011 | T_Acc  0.99 | V_Loss 0.459 | V_Acc 0.917 | 1.715\n",
            "Epoch: 369 | T_Loss 0.012 | T_Acc  0.99 | V_Loss 0.453 | V_Acc 0.917 |  1.71\n",
            "Epoch: 368 | T_Loss 0.011 | T_Acc  0.99 | V_Loss 0.453 | V_Acc 0.917 | 1.696\n",
            "Epoch: 367 | T_Loss 0.012 | T_Acc  0.99 | V_Loss  0.45 | V_Acc 0.917 | 1.963\n",
            "Epoch: 366 | T_Loss 0.012 | T_Acc  0.99 | V_Loss 0.446 | V_Acc 0.917 | 1.811\n",
            "Epoch: 365 | T_Loss 0.013 | T_Acc  0.99 | V_Loss 0.445 | V_Acc 0.917 | 1.671\n",
            "Epoch: 364 | T_Loss 0.012 | T_Acc  0.99 | V_Loss 0.445 | V_Acc 0.917 |  1.64\n",
            "Epoch: 363 | T_Loss 0.013 | T_Acc  0.99 | V_Loss  0.44 | V_Acc 0.917 | 1.743\n",
            "Epoch: 362 | T_Loss 0.014 | T_Acc  0.99 | V_Loss 0.439 | V_Acc 0.917 | 1.889\n",
            "Epoch: 361 | T_Loss 0.012 | T_Acc  0.99 | V_Loss 0.436 | V_Acc 0.917 | 1.573\n",
            "Epoch: 360 | T_Loss 0.013 | T_Acc  0.99 | V_Loss 0.437 | V_Acc 0.917 | 1.642\n",
            "Epoch: 359 | T_Loss 0.013 | T_Acc  0.99 | V_Loss 0.435 | V_Acc 0.917 |  1.68\n",
            "Epoch: 358 | T_Loss 0.014 | T_Acc  0.99 | V_Loss 0.427 | V_Acc 0.917 | 1.929\n",
            "Epoch: 357 | T_Loss 0.014 | T_Acc  0.99 | V_Loss 0.423 | V_Acc 0.917 | 1.656\n",
            "Epoch: 356 | T_Loss 0.014 | T_Acc  0.99 | V_Loss 0.423 | V_Acc 0.917 | 1.654\n",
            "Epoch: 355 | T_Loss 0.015 | T_Acc  0.99 | V_Loss 0.421 | V_Acc 0.917 | 1.662\n",
            "Epoch: 354 | T_Loss 0.014 | T_Acc  0.99 | V_Loss 0.413 | V_Acc 0.917 | 1.937\n",
            "Epoch: 353 | T_Loss 0.016 | T_Acc  0.99 | V_Loss 0.415 | V_Acc 0.917 | 1.549\n",
            "Epoch: 352 | T_Loss 0.016 | T_Acc  0.99 | V_Loss 0.417 | V_Acc 0.917 | 1.644\n",
            "Epoch: 351 | T_Loss 0.017 | T_Acc  0.99 | V_Loss  0.42 | V_Acc 0.917 |  1.65\n",
            "Epoch: 350 | T_Loss 0.016 | T_Acc  0.99 | V_Loss 0.414 | V_Acc 0.917 | 1.888\n",
            "Epoch: 349 | T_Loss 0.017 | T_Acc  0.99 | V_Loss 0.415 | V_Acc 0.917 | 1.779\n",
            "Epoch: 348 | T_Loss 0.017 | T_Acc  0.99 | V_Loss 0.413 | V_Acc 0.917 | 1.636\n",
            "Epoch: 347 | T_Loss 0.017 | T_Acc  0.99 | V_Loss  0.42 | V_Acc 0.917 | 1.546\n",
            "Epoch: 346 | T_Loss 0.019 | T_Acc  0.99 | V_Loss 0.424 | V_Acc 0.917 | 1.612\n",
            "Epoch: 345 | T_Loss 0.019 | T_Acc  0.99 | V_Loss 0.422 | V_Acc 0.917 | 1.896\n",
            "Epoch: 344 | T_Loss 0.019 | T_Acc  0.99 | V_Loss  0.42 | V_Acc 0.917 |  1.57\n",
            "Epoch: 343 | T_Loss 0.018 | T_Acc  0.99 | V_Loss 0.419 | V_Acc 0.917 |   1.6\n",
            "Epoch: 342 | T_Loss 0.018 | T_Acc  0.99 | V_Loss 0.419 | V_Acc 0.917 | 1.597\n",
            "Epoch: 341 | T_Loss 0.017 | T_Acc  0.99 | V_Loss 0.418 | V_Acc 0.917 | 1.891\n",
            "Epoch: 340 | T_Loss 0.017 | T_Acc  0.99 | V_Loss 0.418 | V_Acc 0.917 | 1.614\n",
            "Epoch: 339 | T_Loss 0.019 | T_Acc  0.99 | V_Loss 0.416 | V_Acc 0.917 | 1.578\n",
            "Epoch: 338 | T_Loss 0.019 | T_Acc  0.99 | V_Loss 0.415 | V_Acc 0.917 | 1.588\n",
            "Epoch: 337 | T_Loss 0.019 | T_Acc  0.99 | V_Loss 0.417 | V_Acc 0.917 | 1.657\n",
            "Epoch: 336 | T_Loss 0.021 | T_Acc  0.99 | V_Loss 0.417 | V_Acc 0.917 | 1.906\n",
            "Epoch: 335 | T_Loss 0.018 | T_Acc  0.99 | V_Loss 0.414 | V_Acc 0.917 | 1.592\n",
            "Epoch: 334 | T_Loss 0.021 | T_Acc  0.99 | V_Loss 0.412 | V_Acc 0.917 | 1.479\n",
            "Epoch: 333 | T_Loss  0.02 | T_Acc  0.99 | V_Loss 0.413 | V_Acc 0.917 |  1.59\n",
            "Epoch: 332 | T_Loss  0.02 | T_Acc  0.99 | V_Loss 0.411 | V_Acc 0.917 | 1.859\n",
            "Epoch: 331 | T_Loss 0.021 | T_Acc  0.99 | V_Loss 0.411 | V_Acc 0.917 | 1.582\n",
            "Epoch: 330 | T_Loss  0.02 | T_Acc  0.99 | V_Loss 0.411 | V_Acc 0.917 | 1.581\n",
            "Epoch: 329 | T_Loss 0.019 | T_Acc  0.99 | V_Loss  0.41 | V_Acc 0.917 | 1.493\n",
            "Epoch: 328 | T_Loss 0.024 | T_Acc  0.99 | V_Loss 0.409 | V_Acc 0.917 | 1.614\n",
            "Epoch: 327 | T_Loss 0.022 | T_Acc  0.99 | V_Loss 0.408 | V_Acc 0.917 | 1.839\n",
            "Epoch: 326 | T_Loss  0.02 | T_Acc  0.99 | V_Loss 0.407 | V_Acc 0.917 |  1.57\n",
            "Epoch: 325 | T_Loss 0.022 | T_Acc  0.99 | V_Loss 0.407 | V_Acc 0.917 | 1.556\n",
            "Epoch: 324 | T_Loss 0.022 | T_Acc  0.99 | V_Loss 0.408 | V_Acc 0.917 | 1.562\n",
            "Epoch: 323 | T_Loss 0.024 | T_Acc  0.99 | V_Loss 0.409 | V_Acc 0.917 | 1.894\n",
            "Epoch: 322 | T_Loss 0.026 | T_Acc  0.99 | V_Loss 0.408 | V_Acc 0.917 | 1.612\n",
            "Epoch: 321 | T_Loss 0.023 | T_Acc  0.99 | V_Loss 0.406 | V_Acc 0.917 | 1.503\n",
            "Epoch: 320 | T_Loss 0.023 | T_Acc  0.99 | V_Loss 0.407 | V_Acc 0.917 | 1.471\n",
            "Epoch: 319 | T_Loss 0.022 | T_Acc  0.99 | V_Loss 0.406 | V_Acc 0.917 | 1.526\n",
            "Epoch: 318 | T_Loss 0.024 | T_Acc  0.99 | V_Loss 0.404 | V_Acc 0.917 | 1.812\n",
            "Epoch: 317 | T_Loss 0.025 | T_Acc  0.99 | V_Loss 0.403 | V_Acc 0.917 |  1.55\n",
            "Epoch: 316 | T_Loss 0.026 | T_Acc  0.99 | V_Loss 0.403 | V_Acc 0.917 | 1.533\n",
            "Epoch: 315 | T_Loss 0.026 | T_Acc  0.99 | V_Loss 0.402 | V_Acc 0.917 | 1.436\n",
            "Epoch: 314 | T_Loss 0.025 | T_Acc  0.99 | V_Loss 0.401 | V_Acc 0.917 | 1.777\n",
            "Epoch: 313 | T_Loss 0.026 | T_Acc  0.99 | V_Loss   0.4 | V_Acc 0.917 | 1.798\n",
            "Epoch: 312 | T_Loss 0.026 | T_Acc  0.99 | V_Loss 0.399 | V_Acc 0.917 | 1.529\n",
            "Epoch: 311 | T_Loss 0.027 | T_Acc  0.99 | V_Loss 0.398 | V_Acc 0.917 |  1.53\n",
            "Epoch: 310 | T_Loss 0.029 | T_Acc  0.99 | V_Loss 0.396 | V_Acc 0.917 | 1.504\n",
            "Epoch: 309 | T_Loss  0.03 | T_Acc  0.99 | V_Loss 0.396 | V_Acc 0.917 | 1.824\n",
            "Epoch: 308 | T_Loss 0.028 | T_Acc  0.99 | V_Loss 0.396 | V_Acc 0.917 | 1.534\n",
            "Epoch: 307 | T_Loss 0.028 | T_Acc  0.99 | V_Loss 0.395 | V_Acc 0.917 | 1.449\n",
            "Epoch: 306 | T_Loss  0.03 | T_Acc  0.99 | V_Loss 0.394 | V_Acc 0.917 | 1.515\n",
            "Epoch: 305 | T_Loss 0.031 | T_Acc  0.99 | V_Loss 0.393 | V_Acc 0.917 | 1.406\n",
            "Epoch: 304 | T_Loss 0.028 | T_Acc  0.99 | V_Loss 0.392 | V_Acc 0.917 | 1.742\n",
            "Epoch: 303 | T_Loss 0.031 | T_Acc  0.99 | V_Loss 0.392 | V_Acc 0.917 | 1.514\n",
            "Epoch: 302 | T_Loss  0.03 | T_Acc  0.99 | V_Loss 0.391 | V_Acc 0.917 | 1.485\n",
            "Epoch: 301 | T_Loss 0.032 | T_Acc  0.99 | V_Loss  0.39 | V_Acc 0.917 | 1.501\n",
            "Epoch: 300 | T_Loss 0.032 | T_Acc  0.99 | V_Loss 0.388 | V_Acc 0.917 | 1.515\n",
            "Epoch: 299 | T_Loss 0.032 | T_Acc  0.99 | V_Loss 0.388 | V_Acc 0.917 | 1.721\n",
            "Epoch: 298 | T_Loss 0.031 | T_Acc  0.99 | V_Loss 0.388 | V_Acc 0.917 |  1.47\n",
            "Epoch: 297 | T_Loss 0.032 | T_Acc  0.99 | V_Loss 0.386 | V_Acc 0.917 | 1.486\n",
            "Epoch: 296 | T_Loss 0.031 | T_Acc  0.99 | V_Loss 0.386 | V_Acc 0.917 | 1.498\n",
            "Epoch: 295 | T_Loss 0.033 | T_Acc  0.99 | V_Loss 0.385 | V_Acc 0.917 | 1.596\n",
            "Epoch: 294 | T_Loss 0.032 | T_Acc  0.99 | V_Loss 0.384 | V_Acc 0.917 | 1.712\n",
            "Epoch: 293 | T_Loss 0.034 | T_Acc  0.99 | V_Loss 0.382 | V_Acc 0.917 | 1.454\n",
            "Epoch: 292 | T_Loss 0.034 | T_Acc  0.99 | V_Loss 0.382 | V_Acc 0.917 | 1.437\n",
            "Epoch: 291 | T_Loss 0.036 | T_Acc  0.99 | V_Loss 0.382 | V_Acc 0.917 |  1.43\n",
            "Epoch: 290 | T_Loss 0.036 | T_Acc  0.99 | V_Loss 0.381 | V_Acc 0.917 | 1.641\n",
            "Epoch: 289 | T_Loss 0.036 | T_Acc  0.99 | V_Loss  0.38 | V_Acc 0.917 |  1.69\n",
            "Epoch: 288 | T_Loss 0.036 | T_Acc  0.99 | V_Loss 0.379 | V_Acc 0.917 | 1.423\n",
            "Epoch: 287 | T_Loss 0.037 | T_Acc  0.99 | V_Loss 0.378 | V_Acc 0.917 | 1.433\n",
            "Epoch: 286 | T_Loss 0.037 | T_Acc  0.99 | V_Loss 0.377 | V_Acc 0.917 |  1.35\n",
            "Epoch: 285 | T_Loss 0.039 | T_Acc  0.99 | V_Loss 0.378 | V_Acc 0.917 | 1.674\n",
            "Epoch: 284 | T_Loss 0.038 | T_Acc  0.99 | V_Loss 0.377 | V_Acc 0.917 | 1.645\n",
            "Epoch: 283 | T_Loss 0.041 | T_Acc  0.99 | V_Loss 0.378 | V_Acc 0.917 | 1.411\n",
            "Epoch: 282 | T_Loss 0.041 | T_Acc  0.99 | V_Loss 0.376 | V_Acc 0.917 | 1.419\n",
            "Epoch: 281 | T_Loss  0.04 | T_Acc  0.99 | V_Loss 0.375 | V_Acc 0.917 | 1.392\n",
            "Epoch: 280 | T_Loss 0.039 | T_Acc  0.99 | V_Loss 0.375 | V_Acc 0.917 | 1.633\n",
            "Epoch: 279 | T_Loss  0.04 | T_Acc  0.99 | V_Loss 0.376 | V_Acc 0.917 | 1.651\n",
            "Epoch: 278 | T_Loss 0.041 | T_Acc  0.99 | V_Loss 0.377 | V_Acc 0.917 | 1.368\n",
            "Epoch: 277 | T_Loss  0.04 | T_Acc  0.99 | V_Loss 0.376 | V_Acc 0.917 | 1.362\n",
            "Epoch: 276 | T_Loss 0.041 | T_Acc  0.99 | V_Loss 0.376 | V_Acc 0.917 | 1.396\n",
            "Epoch: 275 | T_Loss 0.042 | T_Acc  0.99 | V_Loss 0.376 | V_Acc 0.917 |  1.63\n",
            "Epoch: 274 | T_Loss  0.04 | T_Acc  0.99 | V_Loss 0.374 | V_Acc 0.917 |  1.63\n",
            "Epoch: 273 | T_Loss 0.042 | T_Acc  0.99 | V_Loss 0.371 | V_Acc 0.917 | 1.395\n",
            "Epoch: 272 | T_Loss 0.044 | T_Acc  0.99 | V_Loss 0.371 | V_Acc 0.917 | 1.392\n",
            "Epoch: 271 | T_Loss 0.044 | T_Acc  0.99 | V_Loss 0.371 | V_Acc 0.917 |  1.39\n",
            "Epoch: 270 | T_Loss 0.045 | T_Acc  0.99 | V_Loss 0.371 | V_Acc 0.917 | 1.454\n",
            "Epoch: 269 | T_Loss 0.044 | T_Acc  0.99 | V_Loss 0.371 | V_Acc 0.917 | 1.613\n",
            "Epoch: 268 | T_Loss 0.044 | T_Acc  0.99 | V_Loss 0.371 | V_Acc 0.917 | 1.371\n",
            "Epoch: 267 | T_Loss 0.045 | T_Acc  0.99 | V_Loss 0.372 | V_Acc 0.917 | 1.384\n",
            "Epoch: 266 | T_Loss 0.046 | T_Acc  0.99 | V_Loss 0.373 | V_Acc 0.917 | 1.346\n",
            "Epoch: 265 | T_Loss 0.045 | T_Acc  0.99 | V_Loss  0.37 | V_Acc 0.917 | 1.351\n",
            "Epoch: 264 | T_Loss 0.045 | T_Acc  0.99 | V_Loss  0.37 | V_Acc 0.917 | 1.582\n",
            "Epoch: 263 | T_Loss 0.045 | T_Acc  0.99 | V_Loss 0.371 | V_Acc 0.917 | 1.344\n",
            "Epoch: 262 | T_Loss 0.045 | T_Acc  0.99 | V_Loss  0.37 | V_Acc 0.917 | 1.295\n",
            "Epoch: 261 | T_Loss 0.046 | T_Acc  0.99 | V_Loss 0.371 | V_Acc 0.917 | 1.329\n",
            "Epoch: 260 | T_Loss 0.045 | T_Acc  0.99 | V_Loss  0.37 | V_Acc 0.917 |  1.27\n",
            "Epoch: 259 | T_Loss 0.045 | T_Acc  0.99 | V_Loss 0.369 | V_Acc 0.917 | 1.581\n",
            "Epoch: 258 | T_Loss 0.047 | T_Acc  0.99 | V_Loss 0.369 | V_Acc 0.917 | 1.423\n",
            "Epoch: 257 | T_Loss 0.048 | T_Acc  0.99 | V_Loss 0.368 | V_Acc 0.917 | 1.331\n",
            "Epoch: 256 | T_Loss 0.047 | T_Acc  0.99 | V_Loss 0.367 | V_Acc 0.917 | 1.311\n",
            "Epoch: 255 | T_Loss 0.047 | T_Acc  0.99 | V_Loss 0.367 | V_Acc 0.917 | 1.319\n",
            "Epoch: 254 | T_Loss 0.048 | T_Acc  0.99 | V_Loss 0.368 | V_Acc 0.917 | 1.552\n",
            "Epoch: 253 | T_Loss 0.047 | T_Acc  0.99 | V_Loss 0.366 | V_Acc 0.917 | 1.531\n",
            "Epoch: 252 | T_Loss 0.047 | T_Acc  0.99 | V_Loss 0.365 | V_Acc 0.917 | 1.318\n",
            "Epoch: 251 | T_Loss 0.049 | T_Acc  0.99 | V_Loss 0.364 | V_Acc 0.917 | 1.313\n",
            "Epoch: 250 | T_Loss 0.048 | T_Acc  0.99 | V_Loss 0.363 | V_Acc 0.917 | 1.323\n",
            "Epoch: 249 | T_Loss  0.05 | T_Acc  0.99 | V_Loss 0.363 | V_Acc 0.917 | 1.254\n",
            "Epoch: 248 | T_Loss 0.048 | T_Acc  0.99 | V_Loss 0.363 | V_Acc 0.917 | 1.528\n",
            "Epoch: 247 | T_Loss 0.048 | T_Acc  0.99 | V_Loss 0.363 | V_Acc 0.917 | 1.297\n",
            "Epoch: 246 | T_Loss  0.05 | T_Acc  0.99 | V_Loss 0.364 | V_Acc 0.917 | 1.307\n",
            "Epoch: 245 | T_Loss  0.05 | T_Acc  0.99 | V_Loss 0.366 | V_Acc 0.917 | 1.228\n",
            "Epoch: 244 | T_Loss 0.049 | T_Acc  0.99 | V_Loss 0.366 | V_Acc 0.917 | 1.209\n",
            "Epoch: 243 | T_Loss 0.051 | T_Acc  0.99 | V_Loss 0.367 | V_Acc 0.917 |  1.51\n",
            "Epoch: 242 | T_Loss 0.049 | T_Acc  0.99 | V_Loss 0.369 | V_Acc 0.917 |  1.49\n",
            "Epoch: 241 | T_Loss  0.05 | T_Acc  0.99 | V_Loss  0.37 | V_Acc 0.917 | 1.273\n",
            "Epoch: 240 | T_Loss 0.051 | T_Acc  0.99 | V_Loss 0.371 | V_Acc 0.917 | 1.267\n",
            "Epoch: 239 | T_Loss 0.051 | T_Acc  0.99 | V_Loss 0.372 | V_Acc 0.917 | 1.273\n",
            "Epoch: 238 | T_Loss 0.051 | T_Acc  0.99 | V_Loss 0.372 | V_Acc 0.917 | 1.273\n",
            "Epoch: 237 | T_Loss 0.052 | T_Acc  0.99 | V_Loss 0.374 | V_Acc 0.917 | 1.474\n",
            "Epoch: 236 | T_Loss 0.051 | T_Acc  0.99 | V_Loss 0.378 | V_Acc 0.917 | 1.286\n",
            "Epoch: 235 | T_Loss 0.052 | T_Acc  0.99 | V_Loss 0.381 | V_Acc 0.917 | 1.304\n",
            "Epoch: 234 | T_Loss 0.053 | T_Acc  0.99 | V_Loss 0.383 | V_Acc 0.917 | 1.234\n",
            "Epoch: 233 | T_Loss 0.052 | T_Acc  0.99 | V_Loss 0.381 | V_Acc 0.917 | 1.257\n",
            "Epoch: 232 | T_Loss 0.053 | T_Acc  0.99 | V_Loss  0.38 | V_Acc 0.917 | 1.458\n",
            "Epoch: 231 | T_Loss 0.053 | T_Acc  0.99 | V_Loss 0.382 | V_Acc 0.917 | 1.457\n",
            "Epoch: 230 | T_Loss 0.055 | T_Acc  0.99 | V_Loss 0.386 | V_Acc 0.917 | 1.233\n",
            "Epoch: 229 | T_Loss 0.056 | T_Acc  0.99 | V_Loss 0.389 | V_Acc 0.917 | 1.247\n",
            "Epoch: 228 | T_Loss 0.055 | T_Acc  0.99 | V_Loss 0.393 | V_Acc 0.917 | 1.225\n",
            "Epoch: 227 | T_Loss 0.057 | T_Acc  0.99 | V_Loss 0.401 | V_Acc 0.833 | 1.239\n",
            "Epoch: 226 | T_Loss 0.058 | T_Acc  0.99 | V_Loss 0.403 | V_Acc 0.833 | 1.472\n",
            "Epoch: 225 | T_Loss 0.058 | T_Acc  0.99 | V_Loss 0.396 | V_Acc 0.833 | 1.432\n",
            "Epoch: 224 | T_Loss  0.06 | T_Acc  0.99 | V_Loss 0.444 | V_Acc 0.833 | 1.198\n",
            "Epoch: 223 | T_Loss 0.065 | T_Acc  0.99 | V_Loss 0.419 | V_Acc 0.833 | 1.196\n",
            "Epoch: 222 | T_Loss 0.069 | T_Acc  0.99 | V_Loss 0.494 | V_Acc  0.75 | 1.217\n",
            "Epoch: 221 | T_Loss 0.078 | T_Acc  0.98 | V_Loss 0.523 | V_Acc  0.75 | 1.234\n",
            "Epoch: 220 | T_Loss 0.071 | T_Acc  0.98 | V_Loss 0.513 | V_Acc 0.833 | 1.414\n",
            "Epoch: 219 | T_Loss 0.087 | T_Acc  0.98 | V_Loss 0.465 | V_Acc 0.833 | 1.365\n",
            "Epoch: 218 | T_Loss 0.097 | T_Acc 0.961 | V_Loss 0.564 | V_Acc  0.75 | 1.203\n",
            "Epoch: 217 | T_Loss 0.108 | T_Acc 0.961 | V_Loss 0.438 | V_Acc 0.833 | 1.205\n",
            "Epoch: 216 | T_Loss 0.124 | T_Acc 0.971 | V_Loss 0.592 | V_Acc 0.583 | 1.195\n",
            "Epoch: 215 | T_Loss 0.115 | T_Acc 0.961 | V_Loss 0.521 | V_Acc  0.75 | 1.183\n",
            "Epoch: 214 | T_Loss   0.2 | T_Acc 0.912 | V_Loss 0.626 | V_Acc 0.667 | 1.381\n",
            "Epoch: 213 | T_Loss 0.359 | T_Acc 0.882 | V_Loss  0.62 | V_Acc  0.75 | 1.209\n",
            "Epoch: 212 | T_Loss  0.14 | T_Acc 0.951 | V_Loss 0.433 | V_Acc 0.833 | 1.193\n",
            "Epoch: 211 | T_Loss 0.143 | T_Acc 0.961 | V_Loss 0.402 | V_Acc 0.917 | 1.198\n",
            "Epoch: 210 | T_Loss 0.192 | T_Acc 0.922 | V_Loss 0.433 | V_Acc 0.917 | 1.511\n",
            "Epoch: 209 | T_Loss  0.24 | T_Acc 0.912 | V_Loss 0.739 | V_Acc  0.75 | 1.173\n",
            "Epoch: 208 | T_Loss 0.272 | T_Acc 0.873 | V_Loss 0.564 | V_Acc  0.75 | 1.393\n",
            "Epoch: 207 | T_Loss 0.281 | T_Acc 0.882 | V_Loss 0.588 | V_Acc 0.833 | 1.164\n",
            "Epoch: 206 | T_Loss 0.198 | T_Acc 0.931 | V_Loss 0.392 | V_Acc 0.917 | 1.179\n",
            "Epoch: 205 | T_Loss 0.111 | T_Acc  0.98 | V_Loss 0.549 | V_Acc 0.833 | 1.155\n",
            "Epoch: 204 | T_Loss 0.103 | T_Acc  0.98 | V_Loss  0.55 | V_Acc 0.833 | 1.155\n",
            "Epoch: 203 | T_Loss 0.127 | T_Acc 0.961 | V_Loss 0.698 | V_Acc  0.75 | 1.152\n",
            "Epoch: 202 | T_Loss 0.136 | T_Acc 0.971 | V_Loss 0.839 | V_Acc 0.667 |  1.36\n",
            "Epoch: 201 | T_Loss  0.14 | T_Acc 0.961 | V_Loss 0.783 | V_Acc 0.667 | 1.328\n",
            "Epoch: 200 | T_Loss 0.155 | T_Acc 0.951 | V_Loss 0.714 | V_Acc  0.75 | 1.397\n",
            "Epoch: 199 | T_Loss 0.168 | T_Acc 0.941 | V_Loss  0.55 | V_Acc  0.75 | 1.264\n",
            "Epoch: 198 | T_Loss 0.257 | T_Acc 0.882 | V_Loss 0.769 | V_Acc 0.667 | 1.136\n",
            "Epoch: 197 | T_Loss 0.347 | T_Acc 0.843 | V_Loss 0.706 | V_Acc 0.667 | 1.273\n",
            "Epoch: 196 | T_Loss 0.314 | T_Acc 0.853 | V_Loss 0.612 | V_Acc  0.75 | 1.326\n",
            "Epoch: 195 | T_Loss 0.436 | T_Acc 0.794 | V_Loss 0.676 | V_Acc 0.667 | 1.134\n",
            "Epoch: 194 | T_Loss 0.331 | T_Acc 0.863 | V_Loss 0.977 | V_Acc 0.583 | 1.131\n",
            "Epoch: 193 | T_Loss 0.176 | T_Acc 0.941 | V_Loss 0.732 | V_Acc 0.667 | 1.131\n",
            "Epoch: 192 | T_Loss 0.184 | T_Acc 0.922 | V_Loss 0.568 | V_Acc  0.75 | 1.121\n",
            "Epoch: 191 | T_Loss 0.183 | T_Acc 0.951 | V_Loss 0.721 | V_Acc 0.667 | 1.129\n",
            "Epoch: 190 | T_Loss 0.201 | T_Acc 0.922 | V_Loss 0.768 | V_Acc 0.667 |  1.28\n",
            "Epoch: 189 | T_Loss 0.197 | T_Acc 0.931 | V_Loss  0.63 | V_Acc  0.75 | 1.131\n",
            "Epoch: 188 | T_Loss 0.207 | T_Acc 0.931 | V_Loss 0.741 | V_Acc 0.667 | 1.106\n",
            "Epoch: 187 | T_Loss 0.218 | T_Acc 0.922 | V_Loss 0.604 | V_Acc  0.75 | 1.072\n",
            "Epoch: 186 | T_Loss 0.255 | T_Acc 0.892 | V_Loss  0.71 | V_Acc  0.75 | 1.103\n",
            "Epoch: 185 | T_Loss 0.261 | T_Acc 0.882 | V_Loss 0.648 | V_Acc 0.667 | 1.635\n",
            "Epoch: 184 | T_Loss 0.234 | T_Acc 0.922 | V_Loss 0.678 | V_Acc  0.75 | 2.964\n",
            "Epoch: 183 | T_Loss 0.335 | T_Acc 0.892 | V_Loss 0.824 | V_Acc 0.833 | 1.107\n",
            "Epoch: 182 | T_Loss 0.563 | T_Acc 0.824 | V_Loss 0.701 | V_Acc 0.583 | 1.081\n",
            "Epoch: 181 | T_Loss 0.175 | T_Acc 0.951 | V_Loss 0.899 | V_Acc 0.583 | 1.079\n",
            "Epoch: 180 | T_Loss 0.131 | T_Acc 0.961 | V_Loss 0.905 | V_Acc   0.5 | 1.071\n",
            "Epoch: 179 | T_Loss  0.17 | T_Acc 0.951 | V_Loss 0.895 | V_Acc 0.583 | 1.281\n",
            "Epoch: 178 | T_Loss 0.153 | T_Acc 0.961 | V_Loss 0.622 | V_Acc  0.75 | 1.264\n",
            "Epoch: 177 | T_Loss 0.143 | T_Acc 0.961 | V_Loss 0.731 | V_Acc 0.667 |  1.03\n",
            "Epoch: 176 | T_Loss  0.17 | T_Acc 0.951 | V_Loss 0.614 | V_Acc 0.667 | 1.081\n",
            "Epoch: 175 | T_Loss 0.189 | T_Acc 0.931 | V_Loss 0.631 | V_Acc 0.667 | 1.062\n",
            "Epoch: 174 | T_Loss 0.181 | T_Acc 0.931 | V_Loss 0.491 | V_Acc  0.75 | 1.029\n",
            "Epoch: 173 | T_Loss 0.206 | T_Acc 0.922 | V_Loss 0.582 | V_Acc 0.667 | 1.073\n",
            "Epoch: 172 | T_Loss 0.254 | T_Acc 0.902 | V_Loss 0.612 | V_Acc 0.667 | 1.257\n",
            "Epoch: 171 | T_Loss 0.279 | T_Acc 0.873 | V_Loss 0.617 | V_Acc 0.667 | 1.226\n",
            "Epoch: 170 | T_Loss 0.325 | T_Acc 0.853 | V_Loss 0.714 | V_Acc 0.583 | 1.067\n",
            "Epoch: 169 | T_Loss 0.378 | T_Acc 0.824 | V_Loss 0.537 | V_Acc  0.75 | 1.034\n",
            "Epoch: 168 | T_Loss 0.284 | T_Acc 0.902 | V_Loss 0.592 | V_Acc 0.583 |   1.0\n",
            "Epoch: 167 | T_Loss 0.258 | T_Acc 0.902 | V_Loss 0.474 | V_Acc 0.833 | 1.035\n",
            "Epoch: 166 | T_Loss 0.301 | T_Acc 0.863 | V_Loss 0.585 | V_Acc  0.75 | 1.057\n",
            "Epoch: 165 | T_Loss 0.378 | T_Acc 0.873 | V_Loss 0.531 | V_Acc  0.75 | 1.217\n",
            "Epoch: 164 | T_Loss 0.289 | T_Acc 0.882 | V_Loss 0.421 | V_Acc  0.75 | 1.155\n",
            "Epoch: 163 | T_Loss 0.306 | T_Acc 0.882 | V_Loss 0.527 | V_Acc  0.75 | 1.032\n",
            "Epoch: 162 | T_Loss 0.352 | T_Acc 0.863 | V_Loss 0.524 | V_Acc  0.75 | 1.048\n",
            "Epoch: 161 | T_Loss 0.397 | T_Acc 0.794 | V_Loss 0.591 | V_Acc  0.75 |  1.04\n",
            "Epoch: 160 | T_Loss 0.369 | T_Acc 0.853 | V_Loss  0.46 | V_Acc  0.75 | 0.973\n",
            "Epoch: 159 | T_Loss 0.434 | T_Acc 0.804 | V_Loss  0.46 | V_Acc  0.75 | 1.065\n",
            "Epoch: 158 | T_Loss 0.455 | T_Acc 0.794 | V_Loss 0.454 | V_Acc 0.833 | 1.166\n",
            "Epoch: 157 | T_Loss 0.461 | T_Acc 0.794 | V_Loss 0.429 | V_Acc 0.833 | 1.086\n",
            "Epoch: 156 | T_Loss 0.451 | T_Acc 0.804 | V_Loss 0.379 | V_Acc 0.833 | 1.008\n",
            "Epoch: 155 | T_Loss 0.408 | T_Acc 0.814 | V_Loss 0.474 | V_Acc  0.75 | 0.923\n",
            "Epoch: 154 | T_Loss 0.414 | T_Acc 0.814 | V_Loss 0.499 | V_Acc  0.75 | 0.979\n",
            "Epoch: 153 | T_Loss 0.438 | T_Acc 0.814 | V_Loss 0.515 | V_Acc  0.75 | 0.971\n",
            "Epoch: 152 | T_Loss 0.476 | T_Acc 0.794 | V_Loss 0.503 | V_Acc  0.75 | 1.002\n",
            "Epoch: 151 | T_Loss 0.492 | T_Acc 0.755 | V_Loss 0.499 | V_Acc  0.75 | 1.157\n",
            "Epoch: 150 | T_Loss 0.521 | T_Acc 0.745 | V_Loss 0.474 | V_Acc  0.75 | 1.129\n",
            "Epoch: 149 | T_Loss 0.542 | T_Acc 0.745 | V_Loss 0.486 | V_Acc 0.833 | 0.926\n",
            "Epoch: 148 | T_Loss 0.596 | T_Acc 0.676 | V_Loss 0.513 | V_Acc 0.833 | 0.935\n",
            "Epoch: 147 | T_Loss 0.641 | T_Acc 0.676 | V_Loss 0.556 | V_Acc 0.667 | 0.976\n",
            "Epoch: 146 | T_Loss 0.585 | T_Acc 0.676 | V_Loss 0.525 | V_Acc 0.833 | 0.913\n",
            "Epoch: 145 | T_Loss 0.606 | T_Acc 0.637 | V_Loss 0.651 | V_Acc 0.667 | 0.955\n",
            "Epoch: 144 | T_Loss 0.599 | T_Acc 0.657 | V_Loss 0.584 | V_Acc  0.75 | 1.161\n",
            "Epoch: 143 | T_Loss 0.608 | T_Acc 0.637 | V_Loss 0.582 | V_Acc  0.75 | 1.126\n",
            "Epoch: 142 | T_Loss 0.615 | T_Acc 0.627 | V_Loss 0.563 | V_Acc  0.75 | 0.921\n",
            "Epoch: 141 | T_Loss 0.599 | T_Acc 0.647 | V_Loss 0.607 | V_Acc 0.667 | 0.954\n",
            "Epoch: 140 | T_Loss 0.601 | T_Acc 0.647 | V_Loss 0.703 | V_Acc 0.583 | 0.903\n",
            "Epoch: 139 | T_Loss 0.601 | T_Acc 0.637 | V_Loss 0.668 | V_Acc 0.583 | 1.005\n",
            "Epoch: 138 | T_Loss 0.599 | T_Acc 0.657 | V_Loss 0.685 | V_Acc 0.583 | 0.918\n",
            "Epoch: 137 | T_Loss 0.608 | T_Acc 0.637 | V_Loss 0.623 | V_Acc 0.667 | 1.044\n",
            "Epoch: 136 | T_Loss 0.605 | T_Acc 0.647 | V_Loss 0.655 | V_Acc 0.667 | 1.105\n",
            "Epoch: 135 | T_Loss 0.624 | T_Acc 0.618 | V_Loss 0.671 | V_Acc 0.583 | 1.072\n",
            "Epoch: 134 | T_Loss 0.601 | T_Acc 0.657 | V_Loss 0.651 | V_Acc 0.667 | 0.884\n",
            "Epoch: 133 | T_Loss  0.61 | T_Acc 0.647 | V_Loss 0.662 | V_Acc 0.667 | 0.922\n",
            "Epoch: 132 | T_Loss 0.618 | T_Acc 0.637 | V_Loss 0.646 | V_Acc 0.667 | 0.864\n",
            "Epoch: 131 | T_Loss 0.611 | T_Acc 0.657 | V_Loss 0.668 | V_Acc 0.583 | 0.838\n",
            "Epoch: 130 | T_Loss 0.613 | T_Acc 0.647 | V_Loss  0.66 | V_Acc 0.583 | 0.851\n",
            "Epoch: 129 | T_Loss 0.618 | T_Acc 0.637 | V_Loss  0.66 | V_Acc 0.583 | 1.013\n",
            "Epoch: 128 | T_Loss 0.615 | T_Acc 0.657 | V_Loss 0.667 | V_Acc 0.583 | 1.072\n",
            "Epoch: 127 | T_Loss 0.622 | T_Acc 0.647 | V_Loss 0.672 | V_Acc 0.583 | 0.924\n",
            "Epoch: 126 | T_Loss 0.624 | T_Acc 0.637 | V_Loss 0.674 | V_Acc 0.583 | 0.828\n",
            "Epoch: 125 | T_Loss 0.622 | T_Acc 0.647 | V_Loss 0.678 | V_Acc 0.583 | 0.832\n",
            "Epoch: 124 | T_Loss 0.625 | T_Acc 0.627 | V_Loss 0.679 | V_Acc 0.583 | 0.828\n",
            "Epoch: 123 | T_Loss 0.622 | T_Acc 0.637 | V_Loss 0.682 | V_Acc 0.583 | 0.834\n",
            "Epoch: 122 | T_Loss 0.626 | T_Acc 0.618 | V_Loss 0.684 | V_Acc 0.583 |  0.85\n",
            "Epoch: 121 | T_Loss  0.63 | T_Acc 0.598 | V_Loss 0.685 | V_Acc   0.5 | 0.906\n",
            "Epoch: 120 | T_Loss 0.632 | T_Acc 0.588 | V_Loss 0.688 | V_Acc   0.5 |  1.02\n",
            "Epoch: 119 | T_Loss 0.636 | T_Acc 0.588 | V_Loss 0.689 | V_Acc   0.5 | 1.017\n",
            "Epoch: 118 | T_Loss 0.639 | T_Acc 0.559 | V_Loss 0.689 | V_Acc   0.5 | 0.833\n",
            "Epoch: 117 | T_Loss 0.645 | T_Acc 0.569 | V_Loss 0.691 | V_Acc   0.5 | 0.833\n",
            "Epoch: 116 | T_Loss 0.648 | T_Acc 0.569 | V_Loss 0.694 | V_Acc   0.5 | 0.824\n",
            "Epoch: 115 | T_Loss 0.585 | T_Acc 0.637 | V_Loss 0.748 | V_Acc   0.5 | 0.836\n",
            "Epoch: 114 | T_Loss 0.498 | T_Acc 0.716 | V_Loss 0.904 | V_Acc   0.5 | 0.785\n",
            "Epoch: 113 | T_Loss 0.481 | T_Acc 0.725 | V_Loss 0.912 | V_Acc   0.5 |  0.82\n",
            "Epoch: 112 | T_Loss 0.506 | T_Acc 0.696 | V_Loss 0.789 | V_Acc   0.5 | 1.004\n",
            "Epoch: 111 | T_Loss 0.498 | T_Acc 0.716 | V_Loss 0.775 | V_Acc   0.5 | 0.991\n",
            "Epoch: 110 | T_Loss 0.515 | T_Acc 0.706 | V_Loss 0.784 | V_Acc   0.5 | 0.818\n",
            "Epoch: 109 | T_Loss 0.513 | T_Acc 0.696 | V_Loss  0.87 | V_Acc 0.583 | 0.778\n",
            "Epoch: 108 | T_Loss 0.511 | T_Acc 0.706 | V_Loss 0.903 | V_Acc 0.583 |  0.79\n",
            "Epoch: 107 | T_Loss 0.501 | T_Acc 0.725 | V_Loss 0.821 | V_Acc 0.583 | 0.821\n",
            "Epoch: 106 | T_Loss 0.513 | T_Acc 0.716 | V_Loss 0.714 | V_Acc 0.583 | 0.766\n",
            "Epoch: 105 | T_Loss 0.515 | T_Acc 0.716 | V_Loss  0.79 | V_Acc 0.583 | 0.791\n",
            "Epoch: 104 | T_Loss 0.521 | T_Acc 0.706 | V_Loss 0.818 | V_Acc 0.583 | 0.829\n",
            "Epoch: 103 | T_Loss  0.53 | T_Acc 0.706 | V_Loss 0.826 | V_Acc 0.583 |  0.96\n",
            "Epoch: 102 | T_Loss 0.514 | T_Acc 0.716 | V_Loss  0.83 | V_Acc 0.583 | 0.946\n",
            "Epoch: 101 | T_Loss 0.549 | T_Acc 0.686 | V_Loss 0.708 | V_Acc 0.583 | 0.758\n",
            "Epoch: 100 | T_Loss 0.536 | T_Acc 0.716 | V_Loss 0.819 | V_Acc 0.583 | 0.807\n",
            "Epoch:  99 | T_Loss 0.541 | T_Acc 0.706 | V_Loss 0.805 | V_Acc 0.583 | 0.745\n",
            "Epoch:  98 | T_Loss 0.571 | T_Acc 0.667 | V_Loss 0.664 | V_Acc 0.583 |   0.8\n",
            "Epoch:  97 | T_Loss 0.578 | T_Acc 0.657 | V_Loss 0.848 | V_Acc   0.5 | 0.736\n",
            "Epoch:  96 | T_Loss 0.553 | T_Acc 0.686 | V_Loss  0.68 | V_Acc 0.583 | 0.745\n",
            "Epoch:  95 | T_Loss 0.669 | T_Acc 0.716 | V_Loss 0.573 | V_Acc 0.667 | 0.811\n",
            "Epoch:  94 | T_Loss 0.649 | T_Acc 0.627 | V_Loss 0.592 | V_Acc 0.667 | 0.917\n",
            "Epoch:  93 | T_Loss 0.642 | T_Acc 0.578 | V_Loss 0.637 | V_Acc 0.583 |   0.9\n",
            "Epoch:  92 | T_Loss 0.683 | T_Acc 0.569 | V_Loss 0.645 | V_Acc 0.583 | 0.724\n",
            "Epoch:  91 | T_Loss  0.59 | T_Acc 0.676 | V_Loss 0.614 | V_Acc 0.667 | 0.728\n",
            "Epoch:  90 | T_Loss  0.46 | T_Acc 0.775 | V_Loss 0.648 | V_Acc 0.667 | 0.713\n",
            "Epoch:  89 | T_Loss 0.323 | T_Acc 0.863 | V_Loss 0.619 | V_Acc 0.667 | 0.725\n",
            "Epoch:  88 | T_Loss 0.361 | T_Acc 0.814 | V_Loss  0.62 | V_Acc 0.667 | 0.723\n",
            "Epoch:  87 | T_Loss 0.378 | T_Acc 0.824 | V_Loss 0.691 | V_Acc 0.583 | 0.791\n",
            "Epoch:  86 | T_Loss 0.453 | T_Acc 0.775 | V_Loss 0.581 | V_Acc 0.667 | 0.761\n",
            "Epoch:  85 | T_Loss 0.611 | T_Acc 0.686 | V_Loss 0.624 | V_Acc  0.75 | 0.916\n",
            "Epoch:  84 | T_Loss 0.461 | T_Acc 0.775 | V_Loss 0.457 | V_Acc  0.75 | 0.905\n",
            "Epoch:  83 | T_Loss 0.368 | T_Acc 0.843 | V_Loss 0.618 | V_Acc 0.667 | 0.813\n",
            "Epoch:  82 | T_Loss 0.333 | T_Acc 0.853 | V_Loss 0.494 | V_Acc  0.75 | 0.693\n",
            "Epoch:  81 | T_Loss 0.361 | T_Acc 0.843 | V_Loss 0.542 | V_Acc  0.75 | 0.713\n",
            "Epoch:  80 | T_Loss 0.394 | T_Acc 0.824 | V_Loss 0.509 | V_Acc  0.75 | 0.713\n",
            "Epoch:  79 | T_Loss 0.405 | T_Acc 0.794 | V_Loss 0.513 | V_Acc  0.75 | 0.676\n",
            "Epoch:  78 | T_Loss 0.437 | T_Acc 0.794 | V_Loss  0.49 | V_Acc  0.75 | 0.679\n",
            "Epoch:  77 | T_Loss 0.449 | T_Acc 0.784 | V_Loss 0.557 | V_Acc 0.667 |  0.68\n",
            "Epoch:  76 | T_Loss 0.397 | T_Acc 0.814 | V_Loss 0.635 | V_Acc 0.667 | 0.677\n",
            "Epoch:  75 | T_Loss 0.398 | T_Acc 0.814 | V_Loss 0.523 | V_Acc 0.667 | 0.848\n",
            "Epoch:  74 | T_Loss 0.366 | T_Acc 0.843 | V_Loss 0.561 | V_Acc 0.667 | 0.859\n",
            "Epoch:  73 | T_Loss 0.378 | T_Acc 0.843 | V_Loss 0.565 | V_Acc 0.667 | 0.703\n",
            "Epoch:  72 | T_Loss 0.378 | T_Acc 0.853 | V_Loss 0.502 | V_Acc  0.75 | 0.687\n",
            "Epoch:  71 | T_Loss 0.403 | T_Acc 0.833 | V_Loss 0.452 | V_Acc 0.833 | 0.656\n",
            "Epoch:  70 | T_Loss 0.407 | T_Acc 0.833 | V_Loss 0.479 | V_Acc  0.75 |  0.66\n",
            "Epoch:  69 | T_Loss 0.395 | T_Acc 0.833 | V_Loss 0.489 | V_Acc  0.75 | 0.671\n",
            "Epoch:  68 | T_Loss 0.406 | T_Acc 0.824 | V_Loss 0.548 | V_Acc 0.667 | 0.627\n",
            "Epoch:  67 | T_Loss 0.431 | T_Acc 0.814 | V_Loss 0.513 | V_Acc  0.75 | 0.664\n",
            "Epoch:  66 | T_Loss  0.41 | T_Acc 0.833 | V_Loss 0.504 | V_Acc  0.75 | 0.661\n",
            "Epoch:  65 | T_Loss 0.433 | T_Acc 0.824 | V_Loss  0.54 | V_Acc  0.75 | 0.867\n",
            "Epoch:  64 | T_Loss 0.438 | T_Acc 0.814 | V_Loss 0.527 | V_Acc  0.75 | 0.785\n",
            "Epoch:  63 | T_Loss 0.475 | T_Acc 0.794 | V_Loss  0.56 | V_Acc 0.667 |  0.78\n",
            "Epoch:  62 | T_Loss 0.449 | T_Acc 0.824 | V_Loss 0.499 | V_Acc  0.75 | 0.646\n",
            "Epoch:  61 | T_Loss 0.484 | T_Acc 0.775 | V_Loss 0.542 | V_Acc 0.667 | 0.592\n",
            "Epoch:  60 | T_Loss 0.487 | T_Acc 0.765 | V_Loss 0.572 | V_Acc 0.667 | 0.629\n",
            "Epoch:  59 | T_Loss 0.529 | T_Acc 0.765 | V_Loss 0.501 | V_Acc  0.75 | 0.614\n",
            "Epoch:  58 | T_Loss 0.517 | T_Acc 0.735 | V_Loss 0.566 | V_Acc 0.667 | 0.631\n",
            "Epoch:  57 | T_Loss   0.5 | T_Acc 0.735 | V_Loss 0.567 | V_Acc 0.667 | 0.673\n",
            "Epoch:  56 | T_Loss 0.521 | T_Acc 0.745 | V_Loss 0.551 | V_Acc 0.667 | 0.644\n",
            "Epoch:  55 | T_Loss  0.55 | T_Acc 0.735 | V_Loss 0.566 | V_Acc 0.667 | 0.649\n",
            "Epoch:  54 | T_Loss 0.665 | T_Acc 0.569 | V_Loss   0.6 | V_Acc 0.667 | 0.762\n",
            "Epoch:  53 | T_Loss 0.566 | T_Acc 0.745 | V_Loss 0.641 | V_Acc 0.583 | 0.759\n",
            "Epoch:  52 | T_Loss 0.535 | T_Acc 0.745 | V_Loss 0.567 | V_Acc 0.667 | 0.736\n",
            "Epoch:  51 | T_Loss 0.532 | T_Acc 0.745 | V_Loss 0.503 | V_Acc  0.75 | 0.615\n",
            "Epoch:  50 | T_Loss 0.538 | T_Acc 0.706 | V_Loss  0.56 | V_Acc 0.667 | 0.574\n",
            "Epoch:  49 | T_Loss  0.55 | T_Acc 0.725 | V_Loss 0.559 | V_Acc 0.667 | 0.566\n",
            "Epoch:  48 | T_Loss 0.537 | T_Acc 0.735 | V_Loss  0.55 | V_Acc 0.667 | 0.613\n",
            "Epoch:  47 | T_Loss 0.551 | T_Acc 0.716 | V_Loss 0.571 | V_Acc 0.667 | 0.568\n",
            "Epoch:  46 | T_Loss 0.528 | T_Acc 0.745 | V_Loss 0.507 | V_Acc  0.75 | 0.573\n",
            "Epoch:  45 | T_Loss 0.557 | T_Acc 0.706 | V_Loss 0.538 | V_Acc  0.75 | 0.594\n",
            "Epoch:  44 | T_Loss 0.576 | T_Acc 0.706 | V_Loss 0.517 | V_Acc  0.75 | 0.558\n",
            "Epoch:  43 | T_Loss 0.599 | T_Acc 0.667 | V_Loss 0.526 | V_Acc  0.75 | 0.557\n",
            "Epoch:  42 | T_Loss 0.594 | T_Acc 0.686 | V_Loss  0.52 | V_Acc  0.75 | 0.698\n",
            "Epoch:  41 | T_Loss 0.611 | T_Acc 0.647 | V_Loss  0.52 | V_Acc  0.75 | 0.747\n",
            "Epoch:  40 | T_Loss 0.642 | T_Acc 0.578 | V_Loss 0.548 | V_Acc  0.75 | 0.711\n",
            "Epoch:  39 | T_Loss 0.618 | T_Acc 0.657 | V_Loss 0.533 | V_Acc  0.75 | 0.597\n",
            "Epoch:  38 | T_Loss 0.595 | T_Acc 0.706 | V_Loss 0.526 | V_Acc  0.75 | 0.533\n",
            "Epoch:  37 | T_Loss 0.599 | T_Acc 0.686 | V_Loss 0.535 | V_Acc  0.75 | 0.594\n",
            "Epoch:  36 | T_Loss 0.612 | T_Acc 0.657 | V_Loss 0.544 | V_Acc  0.75 | 0.563\n",
            "Epoch:  35 | T_Loss 0.682 | T_Acc 0.549 | V_Loss 0.545 | V_Acc  0.75 | 0.535\n",
            "Epoch:  34 | T_Loss 0.628 | T_Acc 0.647 | V_Loss  0.57 | V_Acc  0.75 | 0.528\n",
            "Epoch:  33 | T_Loss 0.638 | T_Acc 0.657 | V_Loss 0.576 | V_Acc 0.667 |  0.55\n",
            "Epoch:  32 | T_Loss 0.634 | T_Acc 0.667 | V_Loss 0.557 | V_Acc  0.75 | 0.537\n",
            "Epoch:  31 | T_Loss 0.631 | T_Acc 0.667 | V_Loss 0.551 | V_Acc  0.75 | 0.553\n",
            "Epoch:  30 | T_Loss 0.632 | T_Acc 0.657 | V_Loss 0.549 | V_Acc  0.75 | 0.672\n",
            "Epoch:  29 | T_Loss 0.636 | T_Acc 0.647 | V_Loss 0.553 | V_Acc  0.75 | 0.664\n",
            "Epoch:  28 | T_Loss 0.637 | T_Acc 0.667 | V_Loss 0.563 | V_Acc  0.75 | 0.673\n",
            "Epoch:  27 | T_Loss 0.641 | T_Acc 0.647 | V_Loss  0.56 | V_Acc  0.75 | 0.532\n",
            "Epoch:  26 | T_Loss 0.646 | T_Acc 0.686 | V_Loss  0.58 | V_Acc 0.667 | 0.505\n",
            "Epoch:  25 | T_Loss 0.648 | T_Acc 0.657 | V_Loss 0.574 | V_Acc  0.75 |   0.5\n",
            "Epoch:  24 | T_Loss 0.651 | T_Acc 0.627 | V_Loss 0.572 | V_Acc  0.75 | 0.523\n",
            "Epoch:  23 | T_Loss 0.655 | T_Acc 0.618 | V_Loss  0.59 | V_Acc 0.667 | 0.494\n",
            "Epoch:  22 | T_Loss 0.654 | T_Acc 0.647 | V_Loss 0.595 | V_Acc  0.75 | 0.481\n",
            "Epoch:  21 | T_Loss 0.658 | T_Acc 0.598 | V_Loss 0.589 | V_Acc  0.75 | 0.482\n",
            "Epoch:  20 | T_Loss 0.668 | T_Acc 0.588 | V_Loss 0.597 | V_Acc 0.667 | 0.493\n",
            "Epoch:  19 | T_Loss 0.664 | T_Acc 0.637 | V_Loss 0.626 | V_Acc 0.667 | 0.482\n",
            "Epoch:  18 | T_Loss 0.673 | T_Acc 0.578 | V_Loss  0.61 | V_Acc  0.75 | 0.475\n",
            "Epoch:  17 | T_Loss 0.674 | T_Acc 0.578 | V_Loss  0.64 | V_Acc 0.667 | 0.508\n",
            "Epoch:  16 | T_Loss 0.674 | T_Acc 0.647 | V_Loss  0.65 | V_Acc 0.583 | 0.589\n",
            "Epoch:  15 | T_Loss 0.685 | T_Acc 0.539 | V_Loss 0.655 | V_Acc  0.75 | 0.608\n",
            "Epoch:  14 | T_Loss 0.682 | T_Acc 0.627 | V_Loss 0.667 | V_Acc 0.583 | 0.604\n",
            "Epoch:  13 | T_Loss 0.684 | T_Acc 0.539 | V_Loss 0.663 | V_Acc 0.833 |  0.48\n",
            "Epoch:  12 | T_Loss 0.691 | T_Acc 0.549 | V_Loss 0.657 | V_Acc 0.583 | 0.462\n",
            "Epoch:  11 | T_Loss 0.688 | T_Acc 0.608 | V_Loss 0.662 | V_Acc 0.667 | 0.452\n",
            "Epoch:  10 | T_Loss 0.691 | T_Acc 0.588 | V_Loss 0.683 | V_Acc 0.667 | 0.451\n",
            "Epoch:   9 | T_Loss 0.692 | T_Acc   0.5 | V_Loss  0.69 | V_Acc 0.583 | 0.448\n",
            "Epoch:   8 | T_Loss 0.692 | T_Acc  0.51 | V_Loss 0.692 | V_Acc   0.5 | 0.477\n",
            "Epoch:   7 | T_Loss 0.693 | T_Acc  0.51 | V_Loss 0.693 | V_Acc   0.5 | 0.438\n",
            "Epoch:   6 | T_Loss 0.693 | T_Acc  0.51 | V_Loss 0.693 | V_Acc   0.5 | 0.451\n",
            "Epoch:   5 | T_Loss 0.693 | T_Acc  0.51 | V_Loss 0.693 | V_Acc   0.5 | 0.432\n",
            "Epoch:   4 | T_Loss 0.693 | T_Acc  0.51 | V_Loss 0.693 | V_Acc   0.5 | 0.441\n",
            "Epoch:   3 | T_Loss 0.693 | T_Acc  0.51 | V_Loss 0.693 | V_Acc   0.5 | 0.456\n",
            "Epoch:   2 | T_Loss 0.693 | T_Acc  0.51 | V_Loss 0.693 | V_Acc   0.5 |  0.54\n",
            "Epoch:   1 | T_Loss 0.693 | T_Acc  0.51 | V_Loss 0.693 | V_Acc   0.5 | 1.304\n",
            "\n",
            " completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì •í™•ë„ ê²€ì¦\n",
        "with torch.no_grad():\n",
        "    test_loss, test_acc = epoch(test_loader, mode = 'test')\n",
        "    test_acc = round(test_acc, 4)\n",
        "    test_loss = round(test_loss, 4)\n",
        "    print('Test Acc.: {}'.format(test_acc))\n",
        "    print('Test Loss: {}'.format(test_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXAaaYNCjo4R",
        "outputId": "ce2e32e1-c17a-4e00-8904-fe4373e6d705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Acc.: 0.5556\n",
            "Test Loss: 3.0819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì˜ìƒ resize ë° ì¶”ì¶œ\n",
        "test_video_name = 'C_3_12_49_BU_DYA_07-29_14-13-29_CE_RGB_DF2_F3'\n",
        "test_video_path = f'/content/drive/MyDrive/Behavior/Abnormal/Theft_Video/{test_video_name}.mp4'\n",
        "cv2.destroyAllWindows()\n",
        "cap = cv2.VideoCapture(test_video_path)\n",
        "img_list = []\n",
        "\n",
        "if cap.isOpened():\n",
        "\n",
        "    while True:\n",
        "        ret, img = cap.read()\n",
        "        if ret:\n",
        "            img = cv2.resize(img, (640, 640))\n",
        "            img_list.append(img)\n",
        "            # cv2_imshow(img)\n",
        "            # cv2.waitKey(1)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print('ì €ì¥ëœ frameì˜ ê°œìˆ˜: {}'.format(len(img_list)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtZOMuDijo6S",
        "outputId": "34d7c2ea-4096-4818-cf51-7bb585c4fa9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì €ì¥ëœ frameì˜ ê°œìˆ˜: 180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Yolov5 + Mediapipe Version\n",
        "\n",
        "net.eval()\n",
        "\n",
        "length = 30 # frame ìƒíƒœë¥¼ í‘œì‹œí•  ê¸¸ì´\n",
        "out_img_list = []\n",
        "dataset = []\n",
        "status = 'None'\n",
        "pose = mp_pose.Pose(static_image_mode=True, model_complexity=1, enable_segmentation=False, min_detection_confidence=n_CONFIDENCE)\n",
        "print('ì‹œí€€ìŠ¤ ë°ì´í„° ë¶„ì„ ì¤‘...')\n",
        "\n",
        "xy_list_list = []\n",
        "for img in tqdm(img_list):\n",
        "    res = yolo_model(img)\n",
        "    res_refine = res.pandas().xyxy[0].values\n",
        "\n",
        "    nms_human = len(res_refine)\n",
        "    if nms_human > 0:\n",
        "        for bbox in res_refine:\n",
        "            xx1, yy1, xx2, yy2 = int(bbox[0])-10, int(bbox[1]), int(bbox[2])+10, int(bbox[3])\n",
        "            if xx1 < 0:\n",
        "                xx1 = 0\n",
        "            elif xx2 > 639:\n",
        "                xx2 = 639\n",
        "            if yy1 < 0:\n",
        "                yy1 = 0\n",
        "            elif yy2 > 639:\n",
        "                yy2 = 639\n",
        "\n",
        "            start_point = (xx1, yy1)\n",
        "            end_point = (xx2, yy2)\n",
        "            if bbox[4] > y_CONFIDENCE:\n",
        "                img = cv2.rectangle(img, start_point, end_point, (0, 0, 255), 2)\n",
        "\n",
        "                c_img = img[yy1:yy2, xx1:xx2]\n",
        "                results = pose.process(cv2.cvtColor(c_img, cv2.COLOR_BGR2RGB)) # ë°”ìš´ë”© box ì•ˆì—ì„œ landmark dot ì¶”ì¶œ\n",
        "                if not results.pose_landmarks: continue\n",
        "                xy_list = []\n",
        "                idx = 0\n",
        "                draw_line_dic = {}\n",
        "                for x_and_y in results.pose_landmarks.landmark:\n",
        "                    if idx in attention_dot:\n",
        "                        xy_list.append(x_and_y.x)\n",
        "                        xy_list.append(x_and_y.y)\n",
        "                        x, y = int(x_and_y.x*(xx2-xx1)), int(x_and_y.y*(yy2-yy1))\n",
        "                        draw_line_dic[idx] = [x, y]\n",
        "                    idx += 1\n",
        "\n",
        "                xy_list_list.append(xy_list)\n",
        "                for line in draw_line:\n",
        "                    x1, y1 = draw_line_dic[line[0]][0], draw_line_dic[line[0]][1]\n",
        "                    x2, y2 = draw_line_dic[line[1]][0], draw_line_dic[line[1]][1]\n",
        "                    c_img = cv2.line(c_img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
        "\n",
        "                if len(xy_list_list) == length:\n",
        "                    dataset = []\n",
        "                    dataset.append({'key' : 0, 'value' : xy_list_list})\n",
        "                    dataset = MyDataset(dataset)\n",
        "                    dataset = DataLoader(dataset)\n",
        "                    xy_list_list = []\n",
        "\n",
        "                    for data, label in dataset:\n",
        "                        data = data.to(device)\n",
        "                        with torch.no_grad():\n",
        "                            result = net(data)\n",
        "                            _, out = torch.max(result, 1)\n",
        "                        if out.item() == 0: status = 'Normal'\n",
        "                        else: status = 'Abnormal'\n",
        "\n",
        "    cv2.putText(img, status, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0, 0, 255), 2)\n",
        "    out_img_list.append(img)\n",
        "# 1ë¶„ì§œë¦¬ ë¶„ì„í•˜ëŠ”ë° 1ë¶„ 52ì´ˆ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czAnU4x-jo8R",
        "outputId": "3771ced5-0431-4d71-cd96-2103b6e09c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì‹œí€€ìŠ¤ ë°ì´í„° ë¶„ì„ ì¤‘...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:21<00:00,  8.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# í…ŒìŠ¤íŠ¸ ì›ë³¸ ì˜ìƒ ë‚´ë³´ë‚´ê¸°\n",
        "filename = '/content/drive/MyDrive/CCTV/Results/output9.mp4'\n",
        "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
        "fps = 3\n",
        "frameSize = (640, 640)\n",
        "isColor = True\n",
        "out = cv2.VideoWriter(filename, fourcc, fps, frameSize, isColor)\n",
        "for out_img in out_img_list:\n",
        "    out.write(out_img)\n",
        "out.release()"
      ],
      "metadata": {
        "id": "S2_SiTKFjo-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ ì €ì¥í•˜ê¸°\n",
        "PATH = '/content/drive/MyDrive/CCTV/Models/'\n",
        "model_name = 'YOLOv5_LSTM.pt'\n",
        "torch.save(net.state_dict(), PATH + model_name)"
      ],
      "metadata": {
        "id": "HxD2sz06jpAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WQfo843jjpCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WIrX7XXcjpFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nj_MSAnXjpHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EcHtdsPDjpJJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}